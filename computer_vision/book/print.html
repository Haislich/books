<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Computer Vision</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../index.html">Introduction</a></li><li class="chapter-item expanded "><a href="image_processing/introduction.html"><strong aria-hidden="true">1.</strong> Image Processing</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/pointwise_operations/introduction.html"><strong aria-hidden="true">1.1.</strong> Pointwise operations</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/pointwise_operations/linear_pointwise_operations.html"><strong aria-hidden="true">1.1.1.</strong> Linear pointwise operations</a></li><li class="chapter-item expanded "><a href="image_processing/pointwise_operations/non_linear_pointwise_operations.html"><strong aria-hidden="true">1.1.2.</strong> Non linear pointwise operations</a></li></ol></li><li class="chapter-item expanded "><a href="image_processing/neighboring_operations/introduction.html"><strong aria-hidden="true">1.2.</strong> Neighboring operations</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/neighboring_operations/local_histogram_equalization.html"><strong aria-hidden="true">1.2.1.</strong> Local histogram equalization</a></li><li class="chapter-item expanded "><a href="image_processing/neighboring_operations/filters/introduction.html"><strong aria-hidden="true">1.2.2.</strong> Filters</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/neighboring_operations/filters/linear_filters.html"><strong aria-hidden="true">1.2.2.1.</strong> Linear Filters</a></li><li class="chapter-item expanded "><a href="image_processing/neighboring_operations/filters/non_linear_filters.html"><strong aria-hidden="true">1.2.2.2.</strong> Non Linear Filters</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="image_processing/image_resizing/introduction.html"><strong aria-hidden="true">1.3.</strong> Image resizing</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/image_resizing/image_sampling.html"><strong aria-hidden="true">1.3.1.</strong> Image sampling</a></li><li class="chapter-item expanded "><a href="image_processing/image_resizing/image_resampling/introduction.html"><strong aria-hidden="true">1.3.2.</strong> Image resampling</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/image_resizing/image_resampling/down_sampling/introduction.html"><strong aria-hidden="true">1.3.2.1.</strong> Down sampling</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/image_resizing/image_resampling/down_sampling/aliasing.html"><strong aria-hidden="true">1.3.2.1.1.</strong> Aliasing</a></li></ol></li><li class="chapter-item expanded "><a href="image_processing/image_resizing/image_resampling/up_sampling/introduction.html"><strong aria-hidden="true">1.3.2.2.</strong> Up sampling</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/image_resizing/image_resampling/up_sampling/interpolation_techniques.html"><strong aria-hidden="true">1.3.2.2.1.</strong> Interpolation techniques</a></li></ol></li></ol></li></ol></li><li class="chapter-item expanded "><a href="image_processing/image_gradient/image_gradiet.html"><strong aria-hidden="true">1.4.</strong> Image Gradient</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/image_gradient/gradient_calculation/gradient_calculation.html"><strong aria-hidden="true">1.4.1.</strong> Gradient Calculation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/image_gradient/gradient_calculation/gradient_magnitude.html"><strong aria-hidden="true">1.4.1.1.</strong> Gradient Magnitude</a></li><li class="chapter-item expanded "><a href="image_processing/image_gradient/gradient_calculation/gradient_orientation.html"><strong aria-hidden="true">1.4.1.2.</strong> Gradient Orientation</a></li><li class="chapter-item expanded "><a href="image_processing/image_gradient/gradient_calculation/gradient_estimation_methods_first_order_operators.html"><strong aria-hidden="true">1.4.1.3.</strong> Gradient Estimation Methods (First-Order Operators)</a></li></ol></li><li class="chapter-item expanded "><a href="image_processing/image_gradient/edges/edges.html"><strong aria-hidden="true">1.4.2.</strong> Edges</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/image_gradient/edges/noise_considerations_in_edge_detection.html"><strong aria-hidden="true">1.4.2.1.</strong> Noise Considerations in Edge Detection</a></li></ol></li><li class="chapter-item expanded "><a href="image_processing/image_gradient/laplacian_filter/laplacian_filter.html"><strong aria-hidden="true">1.4.3.</strong> Laplacian Filter</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/image_gradient/laplacian_filter/derivative_theorem_of_convolution.html"><strong aria-hidden="true">1.4.3.1.</strong> Derivative Theorem of Convolution</a></li><li class="chapter-item expanded "><a href="image_processing/image_gradient/laplacian_filter/laplacian_of_gaussian.html"><strong aria-hidden="true">1.4.3.2.</strong> Laplacian of Gaussian</a></li><li class="chapter-item expanded "><a href="image_processing/image_gradient/laplacian_filter/zero_crossing.html"><strong aria-hidden="true">1.4.3.3.</strong> Zero Crossing</a></li></ol></li><li class="chapter-item expanded "><a href="image_processing/image_gradient/edge_detection/edge_detection.html"><strong aria-hidden="true">1.4.4.</strong> Edge Detection</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/image_gradient/edge_detection/canny_edge_detector.html"><strong aria-hidden="true">1.4.4.1.</strong> Canny Edge Detector</a></li></ol></li></ol></li></ol></li><li class="chapter-item expanded "><a href="fourier_analysis/fourier_analysis.html"><strong aria-hidden="true">2.</strong> Fourier Analysis</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="fourier_analysis/fourier_series.html"><strong aria-hidden="true">2.1.</strong> Fourier Series</a></li><li class="chapter-item expanded "><a href="fourier_analysis/fourier_transform/fourier_transform.html"><strong aria-hidden="true">2.2.</strong> Fourier Transform</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="fourier_analysis/fourier_transform/discrete_fourier_transform.html"><strong aria-hidden="true">2.2.1.</strong> Discrete Fourier Transform</a></li></ol></li><li class="chapter-item expanded "><a href="fourier_analysis/convolution_theorem.html"><strong aria-hidden="true">2.3.</strong> Convolution Theorem</a></li><li class="chapter-item expanded "><a href="fourier_analysis/blurring_in_time_and_frequency_domains.html"><strong aria-hidden="true">2.4.</strong> Blurring in Time and Frequency Domains</a></li><li class="chapter-item expanded "><a href="fourier_analysis/filters.html"><strong aria-hidden="true">2.5.</strong> Filters</a></li><li class="chapter-item expanded "><a href="fourier_analysis/nyquist_sampling_theorem/nyquist_sampling_theorem.html"><strong aria-hidden="true">2.6.</strong> Nyquist Sampling Theorem</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="fourier_analysis/nyquist_sampling_theorem/relation_with_gaussian_pyramids.html"><strong aria-hidden="true">2.6.1.</strong> Relation with Gaussian Pyramids</a></li><li class="chapter-item expanded "><a href="fourier_analysis/nyquist_sampling_theorem/relation_to_human_vision.html"><strong aria-hidden="true">2.6.2.</strong> Relation to Human Vision</a></li></ol></li><li class="chapter-item expanded "><a href="fourier_analysis/hybrid_images.html"><strong aria-hidden="true">2.7.</strong> Hybrid Images</a></li></ol></li><li class="chapter-item expanded "><a href="pyramids/pyramids.html"><strong aria-hidden="true">3.</strong> Pyramids</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="pyramids/gaussian_pyramid.html"><strong aria-hidden="true">3.1.</strong> Gaussian Pyramid</a></li><li class="chapter-item expanded "><a href="pyramids/laplacian_pyramid.html"><strong aria-hidden="true">3.2.</strong> Laplacian Pyramid</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="pyramids/image_reconstruction_with_laplacian_pyramid.html"><strong aria-hidden="true">3.2.1.</strong> Image Reconstruction with Laplacian Pyramid</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="panorama_stitching/panorama_stitching.html"><strong aria-hidden="true">4.</strong> Panorama Stitching</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="panorama_stitching/key_steps_in_panorama_stitching.html"><strong aria-hidden="true">4.1.</strong> Key Steps in Panorama Stitching</a></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision.html"><strong aria-hidden="true">4.2.</strong> Features in Computer Vision</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/common_types_of_features.html"><strong aria-hidden="true">4.2.1.</strong> Common Types of Features</a></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/characteristics_of_a_good_feature.html"><strong aria-hidden="true">4.2.2.</strong> Characteristics of a Good Feature</a></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/local_vs_global_features.html"><strong aria-hidden="true">4.2.3.</strong> Local vs. Global Features</a></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_detectors.html"><strong aria-hidden="true">4.2.4.</strong> Feature Detectors</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_detectors/global_feature_detectors.html"><strong aria-hidden="true">4.2.4.1.</strong> Global Feature Detectors</a></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_detectors/local_feature_detectors.html"><strong aria-hidden="true">4.2.4.2.</strong> Local Feature Detectors</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_detectors/local_feature_detectors/harris_corner_detector.html"><strong aria-hidden="true">4.2.4.2.1.</strong> Harris Corner Detector</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_detectors/local_feature_detectors/harris_corner_detector/small_motion_assumption.html"><strong aria-hidden="true">4.2.4.2.1.1.</strong> Small Motion Assumption</a></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_detectors/local_feature_detectors/harris_corner_detector/harris_corner_detector_algorithm.html"><strong aria-hidden="true">4.2.4.2.1.2.</strong> Harris Corner Detector Algorithm</a></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_detectors/local_feature_detectors/harris_corner_detector/properties_of_the_harris_corner_detector.html"><strong aria-hidden="true">4.2.4.2.1.3.</strong> Properties of the Harris Corner Detector</a></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_detectors/local_feature_detectors/harris_corner_detector/harris_corner_detector_with_scale_invariance.html"><strong aria-hidden="true">4.2.4.2.1.4.</strong> Harris Corner Detector with Scale-Invariance</a></li></ol></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_detectors/local_feature_detectors/blob_detector.html"><strong aria-hidden="true">4.2.4.2.2.</strong> Blob Detector</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_descriptors.html"><strong aria-hidden="true">4.2.5.</strong> Feature Descriptors</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_descriptors/building_histogram_of_oriented_gradients_hog.html"><strong aria-hidden="true">4.2.5.1.</strong> Building Histogram of Oriented Gradients (HoG)</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_descriptors/building_histogram_of_oriented_gradients_hog/image_patches.html"><strong aria-hidden="true">4.2.5.1.1.</strong> Image Patches</a></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_descriptors/building_histogram_of_oriented_gradients_hog/image_gradients.html"><strong aria-hidden="true">4.2.5.1.2.</strong> Image Gradients</a></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_descriptors/building_histogram_of_oriented_gradients_hog/color_histograms.html"><strong aria-hidden="true">4.2.5.1.3.</strong> Color Histograms</a></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_descriptors/building_histogram_of_oriented_gradients_hog/spatial_histograms.html"><strong aria-hidden="true">4.2.5.1.4.</strong> Spatial Histograms</a></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_descriptors/building_histogram_of_oriented_gradients_hog/orientation_normalization.html"><strong aria-hidden="true">4.2.5.1.5.</strong> Orientation Normalization</a></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_descriptors/building_histogram_of_oriented_gradients_hog/hog_algorithm.html"><strong aria-hidden="true">4.2.5.1.6.</strong> HoG Algorithm</a></li></ol></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_descriptors/scale_invariant_feature_transform_sift.html"><strong aria-hidden="true">4.2.5.2.</strong> Scale-Invariant Feature Transform (SIFT)</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_descriptors/scale_invariant_feature_transform_sift/sift_algorithm.html"><strong aria-hidden="true">4.2.5.2.1.</strong> SIFT Algorithm</a></li></ol></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_descriptors/speeded_up_robust_features_surf.html"><strong aria-hidden="true">4.2.5.3.</strong> Speeded Up Robust Features (SURF)</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_descriptors/speeded_up_robust_features_surf/surf_algorithm.html"><strong aria-hidden="true">4.2.5.3.1.</strong> SURF Algorithm</a></li></ol></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_descriptors/binary_descriptors.html"><strong aria-hidden="true">4.2.5.4.</strong> Binary Descriptors</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_descriptors/binary_descriptors/brief_binary_robust_independent_elementary_features.html"><strong aria-hidden="true">4.2.5.4.1.</strong> BRIEF (Binary Robust Independent Elementary Features)</a></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_descriptors/binary_descriptors/orb_oriented_fast_and_rotated_brief.html"><strong aria-hidden="true">4.2.5.4.2.</strong> ORB (Oriented FAST and Rotated BRIEF)</a></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_descriptors/binary_descriptors/freak_fast_retina_keypoint.html"><strong aria-hidden="true">4.2.5.4.3.</strong> FREAK (Fast Retina Keypoint)</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_matching.html"><strong aria-hidden="true">4.2.6.</strong> Feature Matching</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_matching/key_steps_in_feature_matching.html"><strong aria-hidden="true">4.2.6.1.</strong> Key Steps in Feature Matching</a></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_matching/matching_techniques.html"><strong aria-hidden="true">4.2.6.2.</strong> Matching Techniques</a></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/feature_matching/applications_of_feature_matching.html"><strong aria-hidden="true">4.2.6.3.</strong> Applications of Feature Matching</a></li></ol></li><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/deep_learning_approaches.html"><strong aria-hidden="true">4.2.7.</strong> Deep Learning Approaches</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="panorama_stitching/features_in_computer_vision/deep_learning_approaches/superpoint_and_superglue.html"><strong aria-hidden="true">4.2.7.1.</strong> SuperPoint and SuperGlue</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="panorama_stitching/image_transformations.html"><strong aria-hidden="true">4.3.</strong> Image Transformations</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="panorama_stitching/image_transformations/homogeneous_coordinates.html"><strong aria-hidden="true">4.3.1.</strong> Homogeneous Coordinates</a></li><li class="chapter-item expanded "><a href="panorama_stitching/image_transformations/linear_transformations.html"><strong aria-hidden="true">4.3.2.</strong> Linear Transformations</a></li><li class="chapter-item expanded "><a href="panorama_stitching/image_transformations/affine_transformations.html"><strong aria-hidden="true">4.3.3.</strong> Affine Transformations</a></li><li class="chapter-item expanded "><a href="panorama_stitching/image_transformations/homographies.html"><strong aria-hidden="true">4.3.4.</strong> Homographies</a></li><li class="chapter-item expanded "><a href="panorama_stitching/image_transformations/computing_transformations.html"><strong aria-hidden="true">4.3.5.</strong> Computing Transformations</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="panorama_stitching/image_transformations/computing_transformations/computing_translation.html"><strong aria-hidden="true">4.3.5.1.</strong> Computing Translation</a></li><li class="chapter-item expanded "><a href="panorama_stitching/image_transformations/computing_transformations/computing_affine_transformations.html"><strong aria-hidden="true">4.3.5.2.</strong> Computing Affine Transformations</a></li><li class="chapter-item expanded "><a href="panorama_stitching/image_transformations/computing_transformations/computing_homographies.html"><strong aria-hidden="true">4.3.5.3.</strong> Computing Homographies</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="panorama_stitching/image_transformations/computing_transformations/ransac.html"><strong aria-hidden="true">4.3.5.3.1.</strong> RANSAC</a></li></ol></li></ol></li></ol></li><li class="chapter-item expanded "><a href="panorama_stitching/warping.html"><strong aria-hidden="true">4.4.</strong> Warping</a></li><li class="chapter-item expanded "><a href="panorama_stitching/blending.html"><strong aria-hidden="true">4.5.</strong> Blending</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="panorama_stitching/blending/common_blending_techniques.html"><strong aria-hidden="true">4.5.1.</strong> Common Blending Techniques</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/3d_scene_reconstruction.html"><strong aria-hidden="true">5.</strong> 3D Scene Reconstruction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/optical_flow.html"><strong aria-hidden="true">5.1.</strong> Optical Flow</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/optical_flow_vs_motion_field.html"><strong aria-hidden="true">5.1.1.</strong> Optical Flow vs Motion Field</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/examples_of_discrepancies_between_optical_flow_and_motion_field.html"><strong aria-hidden="true">5.1.1.1.</strong> Examples of Discrepancies Between Optical Flow and Motion Field</a></li></ol></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/optical_flow_constraint_equation.html"><strong aria-hidden="true">5.1.2.</strong> Optical Flow Constraint Equation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/derivation_of_the_optical_flow_constraint_equation.html"><strong aria-hidden="true">5.1.2.1.</strong> Derivation of the Optical Flow Constraint Equation</a></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/interpretation_of_the_optical_flow_constraint_equation.html"><strong aria-hidden="true">5.1.2.2.</strong> Interpretation of the Optical Flow Constraint Equation</a></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/aperture_problem.html"><strong aria-hidden="true">5.1.2.3.</strong> Aperture Problem</a></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/lucas_kanade_method.html"><strong aria-hidden="true">5.1.2.4.</strong> Lucas-Kanade Method</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/lucas_kanade_method/conditions_for_a_unique_solution.html"><strong aria-hidden="true">5.1.2.4.1.</strong> Conditions for a Unique Solution</a></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/lucas_kanade_method/coarse_to_fine_flow_estimation.html"><strong aria-hidden="true">5.1.2.4.2.</strong> Coarse-to-Fine Flow Estimation</a></li></ol></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/horn_schunck_method.html"><strong aria-hidden="true">5.1.2.5.</strong> Horn-Schunck Method</a></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/balancing_data_fidelity_and_regularization.html"><strong aria-hidden="true">5.1.2.6.</strong> Balancing Data Fidelity and Regularization</a></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/linearization_and_approximation.html"><strong aria-hidden="true">5.1.2.7.</strong> Linearization and Approximation</a></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/discretization_for_practical_use.html"><strong aria-hidden="true">5.1.2.8.</strong> Discretization for Practical Use</a></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/iterative_solution_algorithm.html"><strong aria-hidden="true">5.1.2.9.</strong> Iterative Solution Algorithm</a></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/robustness_and_extensions.html"><strong aria-hidden="true">5.1.2.10.</strong> Robustness and Extensions</a></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/gradient_descent_for_optimization.html"><strong aria-hidden="true">5.1.2.11.</strong> Gradient Descent for Optimization</a></li></ol></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/optical_flow_estimation_with_deep_learning.html"><strong aria-hidden="true">5.1.3.</strong> Optical Flow Estimation with Deep Learning</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/optical_flow_estimation_with_deep_learning/flownet_and_flownet2.html"><strong aria-hidden="true">5.1.3.1.</strong> FlowNet and FlowNet2</a></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/optical_flow_estimation_with_deep_learning/pwc_net.html"><strong aria-hidden="true">5.1.3.2.</strong> PWC-Net</a></li></ol></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/feature_tracking.html"><strong aria-hidden="true">5.1.4.</strong> Feature Tracking</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="3d_scene_reconstruction/optical_flow/feature_tracking/lucas_kanade_feature_tracker.html"><strong aria-hidden="true">5.1.4.1.</strong> Lucas-Kanade Feature Tracker</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/camera_models.html"><strong aria-hidden="true">5.2.</strong> Camera Models</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="3d_scene_reconstruction/camera_models/intrinsic_parameters.html"><strong aria-hidden="true">5.2.1.</strong> Intrinsic Parameters</a></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/camera_models/extrinsic_parameters.html"><strong aria-hidden="true">5.2.2.</strong> Extrinsic Parameters</a></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/camera_models/pinhole_camera_model.html"><strong aria-hidden="true">5.2.3.</strong> Pinhole Camera Model</a></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/camera_models/forward_imaging_model.html"><strong aria-hidden="true">5.2.4.</strong> Forward Imaging Model</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="3d_scene_reconstruction/camera_models/forward_imaging_model/perspective_projection.html"><strong aria-hidden="true">5.2.4.1.</strong> Perspective Projection</a></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/camera_models/forward_imaging_model/coordinate_transformation.html"><strong aria-hidden="true">5.2.4.2.</strong> Coordinate Transformation</a></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/camera_models/forward_imaging_model/full_projection_matrix.html"><strong aria-hidden="true">5.2.4.3.</strong> Full Projection Matrix</a></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/camera_models/forward_imaging_model/lens_distortion.html"><strong aria-hidden="true">5.2.4.4.</strong> Lens Distortion</a></li></ol></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/camera_models/camera_calibration.html"><strong aria-hidden="true">5.2.5.</strong> Camera Calibration</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="3d_scene_reconstruction/camera_models/camera_calibration/solving_the_overdetermined_system.html"><strong aria-hidden="true">5.2.5.1.</strong> Solving the Overdetermined System</a></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/camera_models/camera_calibration/svd_and_qr_factorization.html"><strong aria-hidden="true">5.2.5.2.</strong> SVD and QR Factorization</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/epipolar_geometry.html"><strong aria-hidden="true">5.3.</strong> Epipolar Geometry</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="3d_scene_reconstruction/epipolar_geometry/key_elements_of_epipolar_geometry.html"><strong aria-hidden="true">5.3.1.</strong> Key Elements of Epipolar Geometry</a></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/epipolar_geometry/essential_matrix.html"><strong aria-hidden="true">5.3.2.</strong> Essential Matrix</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="3d_scene_reconstruction/epipolar_geometry/essential_matrix/properties_of_the_essential_matrix.html"><strong aria-hidden="true">5.3.2.1.</strong> Properties of the Essential Matrix</a></li></ol></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/epipolar_geometry/fundamental_matrix.html"><strong aria-hidden="true">5.3.3.</strong> Fundamental Matrix</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="3d_scene_reconstruction/epipolar_geometry/fundamental_matrix/properties_of_the_fundamental_matrix.html"><strong aria-hidden="true">5.3.3.1.</strong> Properties of the Fundamental Matrix</a></li></ol></li><li class="chapter-item expanded "><a href="3d_scene_reconstruction/epipolar_geometry/8_point_algorithm_for_fundamental_matrix.html"><strong aria-hidden="true">5.3.4.</strong> 8-Point Algorithm for Fundamental Matrix</a></li></ol></li><li class="chapter-item expanded "><a href="stereo_matching/stereo_matching.html"><strong aria-hidden="true">5.4.</strong> Stereo Matching</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="stereo_matching/disparity_and_depth.html"><strong aria-hidden="true">5.4.1.</strong> Disparity and Depth</a></li><li class="chapter-item expanded "><a href="stereo_matching/image_rectification.html"><strong aria-hidden="true">5.4.2.</strong> Image Rectification</a></li><li class="chapter-item expanded "><a href="stereo_matching/local_stereo_matching.html"><strong aria-hidden="true">5.4.3.</strong> Local Stereo Matching</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="stereo_matching/local_stereo_matching/template_matching.html"><strong aria-hidden="true">5.4.3.1.</strong> Template Matching</a></li><li class="chapter-item expanded "><a href="stereo_matching/local_stereo_matching/disparity_space_image_dsi.html"><strong aria-hidden="true">5.4.3.2.</strong> Disparity Space Image (DSI)</a></li><li class="chapter-item expanded "><a href="stereo_matching/local_stereo_matching/stereo_block_matching.html"><strong aria-hidden="true">5.4.3.3.</strong> Stereo Block Matching</a></li></ol></li><li class="chapter-item expanded "><a href="stereo_matching/non_local_stereo_matching.html"><strong aria-hidden="true">5.4.4.</strong> Non-Local Stereo Matching</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="stereo_matching/non_local_stereo_matching/dsi_with_inter_scanline_consistency.html"><strong aria-hidden="true">5.4.4.1.</strong> DSI with Inter-Scanline Consistency</a></li></ol></li><li class="chapter-item expanded "><a href="stereo_matching/stereo_matching_as_an_energy_minimization_problem.html"><strong aria-hidden="true">5.4.5.</strong> Stereo Matching as an Energy Minimization Problem</a></li><li class="chapter-item expanded "><a href="stereo_matching/deep_models_for_stereo_matching.html"><strong aria-hidden="true">5.4.6.</strong> Deep Models for Stereo Matching</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="stereo_matching/deep_models_for_stereo_matching/siamese_networks.html"><strong aria-hidden="true">5.4.6.1.</strong> Siamese Networks</a></li><li class="chapter-item expanded "><a href="stereo_matching/deep_models_for_stereo_matching/dispnet.html"><strong aria-hidden="true">5.4.6.2.</strong> DispNet</a></li><li class="chapter-item expanded "><a href="stereo_matching/deep_models_for_stereo_matching/stereo_mixture_density_networks_smd_nets.html"><strong aria-hidden="true">5.4.6.3.</strong> Stereo Mixture Density Networks (SMD-Nets)</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="stereo_matching/triangulation.html"><strong aria-hidden="true">5.5.</strong> Triangulation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="stereo_matching/triangulation/projection_equation.html"><strong aria-hidden="true">5.5.1.</strong> Projection Equation</a></li><li class="chapter-item expanded "><a href="stereo_matching/triangulation/eliminating_the_scale_factor.html"><strong aria-hidden="true">5.5.2.</strong> Eliminating the Scale Factor</a></li><li class="chapter-item expanded "><a href="stereo_matching/triangulation/system_of_equations.html"><strong aria-hidden="true">5.5.3.</strong> System of Equations</a></li><li class="chapter-item expanded "><a href="stereo_matching/triangulation/solving_the_system.html"><strong aria-hidden="true">5.5.4.</strong> Solving the System</a></li><li class="chapter-item expanded "><a href="stereo_matching/triangulation/handling_noise.html"><strong aria-hidden="true">5.5.5.</strong> Handling Noise</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="deep_learning/deep_learning.html"><strong aria-hidden="true">6.</strong> Deep Learning</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/image_recognition.html"><strong aria-hidden="true">6.1.</strong> Image Recognition</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/image_recognition/image_classification.html"><strong aria-hidden="true">6.1.1.</strong> Image Classification</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/image_recognition/image_classification/first_datasets.html"><strong aria-hidden="true">6.1.1.1.</strong> First Datasets</a></li><li class="chapter-item expanded "><a href="deep_learning/image_recognition/image_classification/early_classifiers_and_approaches.html"><strong aria-hidden="true">6.1.1.2.</strong> Early Classifiers and Approaches</a></li><li class="chapter-item expanded "><a href="deep_learning/image_recognition/image_classification/deep_learning_classifiers_and_approaches.html"><strong aria-hidden="true">6.1.1.3.</strong> Deep Learning Classifiers and Approaches</a></li><li class="chapter-item expanded "><a href="deep_learning/image_recognition/image_classification/top_networks_in_image_classification.html"><strong aria-hidden="true">6.1.1.4.</strong> Top Networks in Image Classification</a></li></ol></li><li class="chapter-item expanded "><a href="deep_learning/image_recognition/semantic_segmentation.html"><strong aria-hidden="true">6.1.2.</strong> Semantic Segmentation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/image_recognition/semantic_segmentation/approach.html"><strong aria-hidden="true">6.1.2.1.</strong> Approach</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/image_recognition/semantic_segmentation/approach/problems_with_standard_fcns.html"><strong aria-hidden="true">6.1.2.1.1.</strong> Problems with Standard FCNs</a></li><li class="chapter-item expanded "><a href="deep_learning/image_recognition/semantic_segmentation/approach/solution_learnable_upsampling.html"><strong aria-hidden="true">6.1.2.1.2.</strong> Solution: Learnable Upsampling</a></li></ol></li><li class="chapter-item expanded "><a href="deep_learning/image_recognition/semantic_segmentation/first_networks.html"><strong aria-hidden="true">6.1.2.2.</strong> First Networks</a></li><li class="chapter-item expanded "><a href="deep_learning/image_recognition/semantic_segmentation/panoptic_segmentation.html"><strong aria-hidden="true">6.1.2.3.</strong> Panoptic Segmentation</a></li></ol></li><li class="chapter-item expanded "><a href="deep_learning/image_recognition/object_detection.html"><strong aria-hidden="true">6.1.3.</strong> Object Detection</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/image_recognition/object_detection/performance_evaluation_intersection_over_union_iou.html"><strong aria-hidden="true">6.1.3.1.</strong> Performance Evaluation: Intersection Over Union (IoU)</a></li><li class="chapter-item expanded "><a href="deep_learning/image_recognition/object_detection/sliding_window_detection.html"><strong aria-hidden="true">6.1.3.2.</strong> Sliding Window Detection</a></li><li class="chapter-item expanded "><a href="deep_learning/image_recognition/object_detection/part_based_models.html"><strong aria-hidden="true">6.1.3.3.</strong> Part-Based Models</a></li><li class="chapter-item expanded "><a href="deep_learning/image_recognition/object_detection/deep_learning_in_object_detection.html"><strong aria-hidden="true">6.1.3.4.</strong> Deep Learning in Object Detection</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/image_recognition/object_detection/region_based_cnn_r_cnn.html"><strong aria-hidden="true">6.1.3.4.1.</strong> Region-Based CNN (R-CNN)</a></li><li class="chapter-item expanded "><a href="deep_learning/image_recognition/object_detection/fast_r_cnn.html"><strong aria-hidden="true">6.1.3.4.2.</strong> Fast R-CNN</a></li><li class="chapter-item expanded "><a href="deep_learning/image_recognition/object_detection/faster_r_cnn.html"><strong aria-hidden="true">6.1.3.4.3.</strong> Faster R-CNN</a></li><li class="chapter-item expanded "><a href="deep_learning/image_recognition/object_detection/feature_pyramid_network_fpn.html"><strong aria-hidden="true">6.1.3.4.4.</strong> Feature Pyramid Network (FPN)</a></li><li class="chapter-item expanded "><a href="deep_learning/image_recognition/object_detection/mask_r_cnn.html"><strong aria-hidden="true">6.1.3.4.5.</strong> Mask R-CNN</a></li></ol></li></ol></li></ol></li><li class="chapter-item expanded "><a href="deep_learning/action_recognition.html"><strong aria-hidden="true">6.2.</strong> Action Recognition</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/action_recognition/deep_learning_for_video_action_recognition.html"><strong aria-hidden="true">6.2.1.</strong> Deep Learning for Video Action Recognition</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/action_recognition/deep_learning_for_video_action_recognition/single_frame_model.html"><strong aria-hidden="true">6.2.1.1.</strong> Single Frame Model</a></li><li class="chapter-item expanded "><a href="deep_learning/action_recognition/deep_learning_for_video_action_recognition/multiple_frames.html"><strong aria-hidden="true">6.2.1.2.</strong> Multiple Frames</a></li><li class="chapter-item expanded "><a href="deep_learning/action_recognition/deep_learning_for_video_action_recognition/limitations_of_feedforward_cnns.html"><strong aria-hidden="true">6.2.1.3.</strong> Limitations of FeedForward CNNs</a></li><li class="chapter-item expanded "><a href="deep_learning/action_recognition/deep_learning_for_video_action_recognition/introduction_of_rnns.html"><strong aria-hidden="true">6.2.1.4.</strong> Introduction of RNNs</a></li><li class="chapter-item expanded "><a href="deep_learning/action_recognition/deep_learning_for_video_action_recognition/subsequent_developments.html"><strong aria-hidden="true">6.2.1.5.</strong> Subsequent Developments</a></li><li class="chapter-item expanded "><a href="deep_learning/action_recognition/deep_learning_for_video_action_recognition/soft_vs_hard_attention.html"><strong aria-hidden="true">6.2.1.6.</strong> Soft vs Hard Attention</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="deep_learning/transformers_in_vision.html"><strong aria-hidden="true">6.3.</strong> Transformers in Vision</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/transformers_in_vision/transformers_in_computer_vision.html"><strong aria-hidden="true">6.3.1.</strong> Transformers in Computer Vision</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/transformers_in_vision/transformers_in_computer_vision/how_to_pass_an_image_to_a_transformer.html"><strong aria-hidden="true">6.3.1.1.</strong> How to Pass an Image to a Transformer?</a></li></ol></li><li class="chapter-item expanded "><a href="deep_learning/transformers_in_vision/transformers_for_object_detection.html"><strong aria-hidden="true">6.3.2.</strong> Transformers for Object Detection</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/transformers_in_vision/transformers_for_object_detection/detr_network_components.html"><strong aria-hidden="true">6.3.2.1.</strong> DETR Network Components</a></li><li class="chapter-item expanded "><a href="deep_learning/transformers_in_vision/transformers_for_object_detection/advantages_of_detr.html"><strong aria-hidden="true">6.3.2.2.</strong> Advantages of DETR</a></li><li class="chapter-item expanded "><a href="deep_learning/transformers_in_vision/transformers_for_object_detection/disadvantages_of_detr.html"><strong aria-hidden="true">6.3.2.3.</strong> Disadvantages of DETR</a></li></ol></li><li class="chapter-item expanded "><a href="deep_learning/transformers_in_vision/transformers_for_semantic_segmentation.html"><strong aria-hidden="true">6.3.3.</strong> Transformers for Semantic Segmentation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/transformers_in_vision/transformers_for_semantic_segmentation/segformer_architecture.html"><strong aria-hidden="true">6.3.3.1.</strong> SegFormer Architecture</a></li><li class="chapter-item expanded "><a href="deep_learning/transformers_in_vision/transformers_for_semantic_segmentation/advantages_of_segformer.html"><strong aria-hidden="true">6.3.3.2.</strong> Advantages of SegFormer</a></li></ol></li><li class="chapter-item expanded "><a href="deep_learning/transformers_in_vision/swin_transformers.html"><strong aria-hidden="true">6.3.4.</strong> Swin Transformers</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/transformers_in_vision/swin_transformers/architecture_and_key_concepts_of_swin_transformers.html"><strong aria-hidden="true">6.3.4.1.</strong> Architecture and Key Concepts of Swin Transformers</a></li><li class="chapter-item expanded "><a href="deep_learning/transformers_in_vision/swin_transformers/advantages_of_swin_transformers.html"><strong aria-hidden="true">6.3.4.2.</strong> Advantages of Swin Transformers</a></li></ol></li><li class="chapter-item expanded "><a href="deep_learning/transformers_in_vision/transformers_for_multi_modal_learning.html"><strong aria-hidden="true">6.3.5.</strong> Transformers for Multi-Modal Learning</a></li></ol></li><li class="chapter-item expanded "><a href="deep_learning/depth_perception.html"><strong aria-hidden="true">6.4.</strong> Depth Perception</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/depth_perception/monocular_depth_estimation.html"><strong aria-hidden="true">6.4.1.</strong> Monocular Depth Estimation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/depth_perception/monocular_depth_estimation/operating_mechanism.html"><strong aria-hidden="true">6.4.1.1.</strong> Operating Mechanism</a></li><li class="chapter-item expanded "><a href="deep_learning/depth_perception/monocular_depth_estimation/challenges_in_monocular_depth_estimation.html"><strong aria-hidden="true">6.4.1.2.</strong> Challenges in Monocular Depth Estimation</a></li><li class="chapter-item expanded "><a href="deep_learning/depth_perception/monocular_depth_estimation/monocular_depth_estimation_techniques.html"><strong aria-hidden="true">6.4.1.3.</strong> Monocular Depth Estimation Techniques</a></li><li class="chapter-item expanded "><a href="deep_learning/depth_perception/monocular_depth_estimation/applications_of_monocular_depth_estimation.html"><strong aria-hidden="true">6.4.1.4.</strong> Applications of Monocular Depth Estimation</a></li><li class="chapter-item expanded "><a href="deep_learning/depth_perception/monocular_depth_estimation/conclusion.html"><strong aria-hidden="true">6.4.1.5.</strong> Conclusion</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="deep_learning/self_supervised_learning.html"><strong aria-hidden="true">6.5.</strong> Self-Supervised Learning</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/self_supervised_learning/main_idea_of_self_supervision.html"><strong aria-hidden="true">6.5.1.</strong> Main Idea of Self-Supervision</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/self_supervised_learning/key_steps_in_self_supervised_learning.html"><strong aria-hidden="true">6.5.1.1.</strong> Key Steps in Self-Supervised Learning</a></li></ol></li><li class="chapter-item expanded "><a href="deep_learning/self_supervised_learning/examples_of_pretext_tasks.html"><strong aria-hidden="true">6.5.2.</strong> Examples of Pretext Tasks</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/self_supervised_learning/examples_of_pretext_tasks/visual_representation_learning_by_context_prediction.html"><strong aria-hidden="true">6.5.2.1.</strong> Visual Representation Learning by Context Prediction</a></li><li class="chapter-item expanded "><a href="deep_learning/self_supervised_learning/examples_of_pretext_tasks/order_recognition.html"><strong aria-hidden="true">6.5.2.2.</strong> Order Recognition</a></li><li class="chapter-item expanded "><a href="deep_learning/self_supervised_learning/examples_of_pretext_tasks/automatic_colorization.html"><strong aria-hidden="true">6.5.2.3.</strong> Automatic Colorization</a></li><li class="chapter-item expanded "><a href="deep_learning/self_supervised_learning/examples_of_pretext_tasks/depth_prediction_in_self_supervision.html"><strong aria-hidden="true">6.5.2.4.</strong> Depth Prediction in Self-Supervision</a></li></ol></li><li class="chapter-item expanded "><a href="deep_learning/self_supervised_learning/advantages_of_self_supervision.html"><strong aria-hidden="true">6.5.3.</strong> Advantages of Self-Supervision</a></li><li class="chapter-item expanded "><a href="deep_learning/self_supervised_learning/contrastive_learning.html"><strong aria-hidden="true">6.5.4.</strong> Contrastive Learning</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deep_learning/self_supervised_learning/contrastive_learning/key_concepts_in_contrastive_learning.html"><strong aria-hidden="true">6.5.4.1.</strong> Key Concepts in Contrastive Learning</a></li><li class="chapter-item expanded "><a href="deep_learning/self_supervised_learning/contrastive_learning/momentum_contrast_moco.html"><strong aria-hidden="true">6.5.4.2.</strong> Momentum Contrast (MoCo)</a></li><li class="chapter-item expanded "><a href="deep_learning/self_supervised_learning/contrastive_learning/simclr_simple_framework_for_contrastive_learning_of_visual_representations.html"><strong aria-hidden="true">6.5.4.3.</strong> SimCLR (Simple Framework for Contrastive Learning of Visual Representations)</a></li><li class="chapter-item expanded "><a href="deep_learning/self_supervised_learning/contrastive_learning/barlow_twins.html"><strong aria-hidden="true">6.5.4.4.</strong> Barlow Twins</a></li><li class="chapter-item expanded "><a href="deep_learning/self_supervised_learning/contrastive_learning/key_ideas_of_barlow_twins.html"><strong aria-hidden="true">6.5.4.5.</strong> Key Ideas of Barlow Twins</a></li><li class="chapter-item expanded "><a href="deep_learning/self_supervised_learning/contrastive_learning/applications_of_barlow_twins.html"><strong aria-hidden="true">6.5.4.6.</strong> Applications of Barlow Twins</a></li><li class="chapter-item expanded "><a href="deep_learning/self_supervised_learning/contrastive_learning/advantages_of_barlow_twins.html"><strong aria-hidden="true">6.5.4.7.</strong> Advantages of Barlow Twins</a></li></ol></li></ol></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Computer Vision</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>This book is the result of the lesson of Vision and Perception at University Sapienza of Rome, in my MsC in artificial Intelligence and robotics.</p>
<h2 id="the-need-of-computer-vision"><a class="header" href="#the-need-of-computer-vision">The need of computer vision</a></h2>
<p>The goal of computer vision is to enable computers and systems to derive meaningful information from digital images, videos, and other visual inputs, and to act or make decisions based on that information.
Essentially, it aims to replicate and automate the capabilities of the human visual system.</p>
<h2 id="computer-vision-vs-computer-graphics"><a class="header" href="#computer-vision-vs-computer-graphics">Computer Vision vs. Computer Graphics</a></h2>
<p>Computer Vision is about interpreting and understanding the content of an image or video.
It aims to replicate the human visual system to recognize, detect, and understand objects and scenes in images or videos.</p>
<p>Computer Graphics is the creation and manipulation of images and videos using computers.
It focuses on generating visual content through rendering, animation, and visualization techniques.
Essentially, while computer vision is about understanding images, computer graphics is about creating them.</p>
<h2 id="computer-vision-vs-image-processing"><a class="header" href="#computer-vision-vs-image-processing">Computer Vision vs. Image Processing</a></h2>
<p>Image Processing involves performing operations on images to enhance them or extract useful information.
It deals with basic transformations like resizing, filtering, and color adjustments, and is often used as a preprocessing step in computer vision workflows to improve the quality of input data for better analysis.</p>
<p>Computer Vision uses the outputs of image processing to perform more complex tasks such as object recognition, scene reconstruction, and more.
It goes beyond processing the image to understanding the context and elements within the visual data.</p>
<h2 id="computer-vision-vs-deep-learning"><a class="header" href="#computer-vision-vs-deep-learning">Computer Vision vs. Deep Learning</a></h2>
<p>Deep Learning is a subset of machine learning that uses neural networks with many layers (hence "deep") to learn from a vast amount of data.
Deep learning algorithms, particularly Convolutional Neural Networks (CNNs), have become fundamental in modern computer vision tasks, enabling advancements in recognition, detection, and classification.</p>
<p>Computer Vision is one of the fields that has greatly benefited from deep learning, applying these algorithms to achieve significant improvements in visual understanding and analysis.
However, computer vision is not limited to deep learning alone and includes a variety of techniques from image processing, pattern recognition, and geometric modeling.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-processing"><a class="header" href="#image-processing">Image Processing</a></h1>
<p>A general image processing operator is a function that takes one or more input images and procudes an output image.</p>
<p>An image can be thought as a function $f$ from $\mathcal{R}^2$ to $\mathcal{R}^3$.
Assume you have a point $p \in \mathcal{R}^2$:</p>
<p>$$
f(p) = \begin{bmatrix} r(p) \ g(p) \ b(p) \end{bmatrix}
$$</p>
<p>Where $r,g$ and $b$ are rispectively the reed, green and blue components of the image.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pointwise-operations"><a class="header" href="#pointwise-operations">Pointwise operations</a></h1>
<p>Pointwise operations are applied at a pixel level and the output only depends on the input pixel and some additional informations.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="linear-pointwise-operations"><a class="header" href="#linear-pointwise-operations">Linear pointwise operations</a></h1>
<p>A common linear point process is multiplication and addition with a constant, which regulates the contrast and the brightness respectively.</p>
<p>Another linear operation is the linear blend, which is used to create a temporal cross-dissolve effect between two images or videos.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="non-linear-pointwise-operations"><a class="header" href="#non-linear-pointwise-operations">Non linear pointwise operations</a></h1>
<h2 id="gamma-corection"><a class="header" href="#gamma-corection">Gamma corection</a></h2>
<p>Gamma correction is a non linear transform that is often applied to images before further processing.</p>
<p>$$
p_{out} = p_{in}^{\gamma}
$$</p>
<p>Where $\gamma \approx 2.2$, which is in general not a good approximation but works well in practice.
Gamma correction is used to align digital images to human visual perception, improving image quality and giving the image consistency accross different devices.</p>
<h2 id="histrogram-equalization"><a class="header" href="#histrogram-equalization">Histrogram equalization</a></h2>
<p>The histogram of each chanel and luminance values of an image describes the set of intensity values in an image.
From the histogram of an image we can compute relevan statistics for the image, in particular we can determine the pixel intensity distribution.</p>
<p>Histogram equalization is a technique used to automatically determine the best contrast values in an image by adjusting the distribution of pixel instensity values of an image such that the histogram of pixel instensities is more uniform across the possible instisity values.</p>
<p>Histogram equalization is a global operation because it considers the distribution of pixel intensities across the entire image.
The adjustment made to each pixel's intensity is based on the cumulative distribution function (CDF) derived from the global histogram of all the pixel values in the image.
The histogram equalization is considered a pointwise operation because each pixel's new value is determined solely by its original value, without considering the values of neighboring pixels.
The mapping function, derived from the CDF, is applied directly to each pixel to determine its new intensity.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="neighboring-operations"><a class="header" href="#neighboring-operations">Neighboring operations</a></h1>
<p>Neighbourhood operations are a generalization of the point operations.
A pixel in the processed image now depends not only on the corresponding pixel in the input image but also its neighbouring pixels.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="local-histogram-equalization"><a class="header" href="#local-histogram-equalization">Local histogram equalization</a></h1>
<p>Local histogram equalization is a technique used to enhance the contrast of images, similar to global histogram equalization, but with a key difference: it operates on small, localized regions of the image rather than on the entire image.</p>
<p>This method is particularly useful for improving the visibility of features in images that have varying lighting conditions across different areas.</p>
<p>In local histogram equalization, the resulting intensity values for a pixel are determined by the histogram of the region it belongs to, making it a neighborhood operation.
Each pixel's new intensity depends on the specific distribution of intensities within its local neighborhood or region, as defined by the small tiles or windows used in the process.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="filters"><a class="header" href="#filters">Filters</a></h1>
<p>Filters are types of neighborhood operations designed to modify or enhance images through a defined set of rules or kernels.
Filters can be linear (like Gaussian or averaging filters) or non-linear (like median filters), each tailored to specific tasks such as smoothing, sharpening, or noise reduction.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="linear-filters"><a class="header" href="#linear-filters">Linear Filters</a></h1>
<p>Linear Filters apply a linear operation to the pixels in a neighborhood defined by a kernel or a mask.
The new value of a target pixel is computed as a weighted sum of the pixel values within the kernel's footprint.</p>
<p>Linear filters are typically applied using a process called convolution, which is closely related to another operation known as cross-correlation.
Both convolution and cross-correlation involve sliding a kernel (or filter) over the image and computing the sum of element-wise products at each position.
The subtle difference between these two operations primarily lies in the orientation of the kernel:</p>
<ul>
<li>
<p>Cross-Correlation: In cross-correlation, the kernel is slid over the image as it is, without flipping. This means the top-left value of the kernel always multiplies the top-left value of the image region it covers.</p>
</li>
<li>
<p>Convolution: Convolution involves flipping the kernel both horizontally and vertically before sliding it over the image. This means that the operation is a more complex form of template matching, as it takes into account the spatial orientation of the kernel's weights relative to the target feature in the image.</p>
</li>
</ul>
<p>Convolution is used in signal processing and image processing because it mathematically models the way physical systems respond to stimuli.
In practical image processing, especially with symmetric kernels (like Gaussian blurs), convolution and cross-correlation can produce the same results because the flipping of a symmetric kernel does not change its layout.</p>
<p>Convolution is a key operation used in convolutional neural networks (CNNs), including famous architectures like AlexNet.
CNNs fundamentally use a sequence of learned linear filters (convolutional layers) to process the input images.
This operation allows CNNs to effectively learn features from image data, making them particularly well-suited for tasks like image classification, object detection, and more.
Convolutional layers in a CNN use learned filters (kernels) to convolve across the input image or feature maps from previous layers.
Each filter is designed (or learned during training) to detect specific features such as edges, textures, or more complex patterns depending on the depth in the network.
As the input data passes through successive convolutional layers, the network can form a hierarchy of features from simple to complex.
Lower layers typically detect simple features (e.g., edges and corners), while deeper layers combine these features to detect higher-level structures (e.g., parts of objects or entire objects).</p>
<p>In the context of convolutional neural networks (CNNs), padding and stride are crucial parameters that influence how the convolution operation is applied to the input image or feature maps. Both settings help control the spatial dimensions of the output feature maps and can have a significant impact on the network's performance and efficiency.</p>
<p>Padding refers to the practice of adding layers of zeros (or other values) around the edge of the input image or feature map before applying the convolution operation.
Without padding, the spatial dimensions of the output feature map are reduced with each convolutional layer (unless a stride of 1 is used and the kernel size is 1x1).
Padding allows the convolution operation to cover the bordering elements of the input, enabling the output feature map to maintain the same size as the input.
This is particularly important in deep networks, where many convolutional layers would otherwise progressively shrink the spatial dimensions of the feature maps to a point where no further convolutions could be applied.
Without padding, pixels on the border of the image are used less frequently than pixels in the center when applying filters.
Padding increases the number of times edge pixels are used in convolution computations, helping the model learn from the entire image more effectively.</p>
<p>Stride dictates the number of pixels the convolution filter moves across the input image or feature map after each operation.
A stride greater than one reduces the spatial dimensions of the output feature map.
This is because the filter skips over pixels as it slides across the input.
Using a stride of 2, for example, typically reduces the dimensions of the output feature map to half those of the input, assuming other settings remain constant.
Increasing the stride reduces the computational load and the size of the output, which can speed up the training and inference processes.
A higher stride increases the effective field of view of each application of the convolution kernel, allowing it to cover a broader area of the input with fewer operations.
This can help the network capture more global features faster, though it may reduce the granularity of the feature maps.</p>
<h2 id="gaussian-filters"><a class="header" href="#gaussian-filters">Gaussian Filters</a></h2>
<p>Gaussian filters are a type of image smoothing filter that reduce noise and detail in images using a Gaussian function.
They are characterized by their bell-shaped curve in one dimension and by a surface whose sections are Gaussian curves in two dimensions.
Gaussian filters are widely used due to their properties in the spatial and frequency domains.
Due to its nature, the Gaussian filter provides smooth gradients without sharp transitions, making it ideal for blurring and for use in scale-space representation.
In two dimensions, a Gaussian filter is circularly symmetric (isotropic), meaning it blurs uniformly in all directions.
Gaussian filters have a low-pass characteristic, which means they attenuate high-frequency components more than low-frequency components, effectively reducing image noise and detail.</p>
<p>A separable filter is a type of filter that can be broken down into the product of two or more one-dimensional kernels.
This allows the two-dimensional convolution operation to be performed more efficiently by reducing it to multiple one-dimensional convolutions.</p>
<p>Gaussian filters are inherently separable, which is one of their advantageous properties.
A two-dimensional Gaussian kernel can be expressed as the outer product of two identical one-dimensional Gaussian kernels.
This means that a 2D Gaussian blurring operation can be efficiently implemented by first applying a 1D Gaussian blur vertically across the image and then horizontally.
This separability makes Gaussian filters particularly appealing for real-time processing and applications in computer vision where computational resources are a concern.</p>
<h2 id="sharpen-filter"><a class="header" href="#sharpen-filter">Sharpen Filter</a></h2>
<p>Sharpening filters are used in image processing to enhance the visibility of edges and fine details in images.
They work by emphasizing high-frequency components, which correspond to rapid changes in image intensity, such as edges.
This is typically achieved through a process that enhances the contrast at these high-frequency locations.</p>
<p>A common approach involves using a kernel that accentuates edges.
This can be done with kernels that approximate the second derivative of the image (like the Laplacian filter), or by simply using a kernel that boosts center pixel intensity while subtracting a fraction of the neighboring pixel intensities.</p>
<p>Gaussian filters are inherently smoothing filters and are typically used to reduce noise and detail in the images.
When used as part of an unsharp masking technique, the Gaussian filter serves to isolate the low-frequency components by smoothing out the high frequencies.
The subtractive process in unsharp masking that follows the Gaussian blurring emphasizes the high-frequency components (edges) by reducing the weight of low frequencies.
This process effectively increases the sharpness of the image.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="non-linear-filters"><a class="header" href="#non-linear-filters">Non Linear Filters</a></h1>
<p>Non-Linear Filters involve operations where the output is not a linear combination of the input pixel values. The operation may include conditions, thresholds, or more complex relationships that do not satisfy linearity.</p>
<h2 id="median-filters"><a class="header" href="#median-filters">Median Filters</a></h2>
<p>Median filters are a type of non-linear digital filtering technique, often used to remove noise from an image or signal.
The median filter is particularly effective at removing 'salt and pepper' noise while preserving edges in an image.
The median filter operates by sliding a window (kernel) over each pixel of the image. For each position of the window, the pixel values covered by the window are sorted numerically. The median value of the sorted pixels is then determined, and this median value replaces the pixel value at the center of the window.
Unlike mean filtering, median filtering does not blur the edges, as the median is less sensitive than the mean to extreme values (which are often edge pixels).</p>
<h2 id="bilateral-filter"><a class="header" href="#bilateral-filter">Bilateral Filter</a></h2>
<p>Bilateral filtering is an advanced method of image smoothing that provides edge preservation while reducing noise, addressing a common limitation found in traditional Gaussian filters.</p>
<p>Gaussian filters are excellent for blurring and noise reduction in images because they effectively smooth variations in intensity.
They work by applying a Gaussian kernel to the image, which averages the pixel values in a way that nearby pixels have a larger influence on the output than distant ones.
However, this isotropic smoothing does not discriminate between edges and noise; it blurs everything indiscriminately, including important edge details.
As a result, while Gaussian filters reduce noise, they also tend to blur sharp edges, which can be undesirable in applications where maintaining edge clarity is important.</p>
<p>Bilateral filtering was introduced to overcome the edge-blurring problem of Gaussian filters.
It does this by taking both spatial proximity and the intensity similarity into account when performing the smoothing:</p>
<ul>
<li>
<p>Spatial Component: Like Gaussian filtering, bilateral filtering considers the closeness of pixels. This is typically modeled using a Gaussian distribution that decreases the weights of pixels based on their spatial distance from the target pixel.</p>
</li>
<li>
<p>Range Component: Unlike Gaussian filtering, bilateral filtering also considers the similarity in intensity values. This range filter ensures that only pixels with intensity values similar to the target pixel are considered for averaging. This component is also typically modeled using a Gaussian distribution, but instead of spatial distance, it uses the intensity difference.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-resizing"><a class="header" href="#image-resizing">Image resizing</a></h1>
<p>Image resizing is a fundamental operation in image processing that involves changing the dimensions of an image.
This can mean scaling up to make the image larger, or scaling down to make it smaller.
Effective image resizing techniques are crucial for a wide range of applications, from multimedia processing to machine learning, where images need to be normalized to a consistent size.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-sampling"><a class="header" href="#image-sampling">Image sampling</a></h1>
<p>Sampling refers to the process of selecting a subset of data from a continuous signal or from a larger set of data points. In the context of image processing, this often relates to the initial acquisition of digital data from an analog source (like converting light captured by a camera sensor into pixel values) or selecting specific pixels or regions from an existing digital image for analysis or processing.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-resampling"><a class="header" href="#image-resampling">Image resampling</a></h1>
<p>Resampling is about modifying an existing dataset (changing the pixel grid of an image) through processes like interpolation or decimation.
This typically involves creating a new set of data points from the original dataset under conditions where you're changing the grid, spacing, or orientation of the data.</p>
<p>Resampling is used when the existing dataset’s spatial arrangement doesn’t fit the needed application, such as when scaling images to a new resolution, rotating them, or applying other geometric transformations that alter their original grid structure.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="down-sampling"><a class="header" href="#down-sampling">Down sampling</a></h1>
<p>Downsampling is a resampling process that involves reducing the number of pixels, commonly for reducing file size, removing redundant information, or adjusting the image to a lower resolution display.
Downsampling also uses methods like averaging or more complex filters to decide the value of new, fewer pixels based on the original pixel grid.
A common way of woind down sampling is subsampling</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="aliasing"><a class="header" href="#aliasing">Aliasing</a></h1>
<p>Aliasing is a phenomenon that occurs when a signal is sampled without sufficient resolution to accurately capture its high-frequency components.
In the context of image processing, aliasing manifests as visual artifacts—such as jagged edges or moiré patterns—when high-frequency details (fine textures or sharp transitions) are not adequately represented due to undersampling.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="up-sampling"><a class="header" href="#up-sampling">Up sampling</a></h1>
<p>Upsampling is a specific type of resampling where the goal is to increase the number of pixels in an image, typically to make the image larger or to match a certain resolution requirement.
This is done through interpolation methods such as nearest neighbor, bilinear, bicubic, etc., which calculate and fill in the pixel values that didn’t exist in the original image.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="interpolation-techniques"><a class="header" href="#interpolation-techniques">Interpolation techniques</a></h1>
<h2 id="nearest-neighbor"><a class="header" href="#nearest-neighbor">Nearest Neighbor</a></h2>
<p>Nearest neighbor interpolation is a method of upsampling that interpolates new pixel values depending only on the closest pixels.
Is one of the fastes and simplest methods that can be used.</p>
<h2 id="bilinear-interpolation"><a class="header" href="#bilinear-interpolation">Bilinear interpolation</a></h2>
<p>Bilinear interpolation is a resampling method used in image processing to determine the intensity of a new pixel based on a weighted average of the $4$ nearest pixels in the original image.</p>
<h2 id="bicubic-interpolation"><a class="header" href="#bicubic-interpolation">Bicubic interpolation</a></h2>
<p>Bilinear interpolation is a resampling method used in image processing to determine the intensity of a new pixel based on a weighted average of the $16$ nearest pixels in the original image.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-gradient"><a class="header" href="#image-gradient">Image Gradient</a></h1>
<p>An image gradient represents the directional change in intensity or color within an image. It measures how the image intensity changes between adjacent pixels and is often used to detect edges and other significant transitions.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gradient-calculation"><a class="header" href="#gradient-calculation">Gradient Calculation</a></h1>
<p>The gradient of an image is computed by taking derivatives in both the horizontal and vertical directions. Typically, this is done by convolving the image with derivative filters (kernels). The gradient consists of two components:</p>
<ul>
<li><strong>Horizontal Gradient</strong> ((G_x)): Measures changes in intensity in the horizontal direction.</li>
<li><strong>Vertical Gradient</strong> ((G_y)): Measures changes in intensity in the vertical direction.</li>
</ul>
<p>Partial derivatives represent the rate of change of image intensities with respect to the (x) and (y) directions independently. These derivatives provide insight into how pixel values vary across each dimension of the image.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gradient-magnitude"><a class="header" href="#gradient-magnitude">Gradient Magnitude</a></h1>
<p>The magnitude of the gradient at a pixel measures the strength of intensity changes, indicating the presence of edges or boundaries. High gradient magnitudes typically signal strong edges where intensity changes abruptly, while low values suggest flatter regions.</p>
<p>[
\text{Magnitude} (G) = \sqrt{G_x^2 + G_y^2}
]</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gradient-orientation"><a class="header" href="#gradient-orientation">Gradient Orientation</a></h1>
<p>The orientation of the gradient reveals the direction of the greatest rate of increase in intensity from one pixel to another. It is perpendicular to the edge direction and provides the angle at which intensity changes occur, aiding in feature detection, such as in texture analysis or object recognition.</p>
<p>[
\text{Orientation} (\theta) = \text{atan2}(G_y, G_x)
]</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gradient-estimation-methods-first-order-operators"><a class="header" href="#gradient-estimation-methods-first-order-operators">Gradient Estimation Methods (First-Order Operators)</a></h1>
<ul>
<li><strong>Sobel Operator</strong>: Utilizes separate convolution kernels for detecting horizontal ((G_x)) and vertical ((G_y)) changes.</li>
<li><strong>Prewitt Operator</strong>: Similar to the Sobel operator but with different kernels that don't emphasize pixels directly adjacent to the central pixel.</li>
<li><strong>Scharr Operator</strong>: Offers optimized rotation invariance and better derivative approximation.</li>
<li><strong>Roberts Operator</strong>: Employs a 2x2 kernel pair, effective for detecting diagonal edges.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="edges"><a class="header" href="#edges">Edges</a></h1>
<p>Edges are points where the image brightness changes abruptly, often indicating the boundaries of objects or significant variations in the scene. They are critical in image processing for tasks like segmentation and object recognition. Image gradients are directly related to edge detection since they emphasize regions with notable intensity changes.</p>
<ul>
<li><strong>Gradient Magnitude</strong>: High magnitudes indicate strong intensity changes, likely marking the location of edges.</li>
<li><strong>Gradient Orientation</strong>: Points in the direction of the steepest intensity increase and is perpendicular to the edge direction.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="noise-considerations-in-edge-detection"><a class="header" href="#noise-considerations-in-edge-detection">Noise Considerations in Edge Detection</a></h1>
<p>Noise can create false edges or obscure true edges. Preprocessing with a Gaussian filter to reduce noise, as used in the Laplacian of Gaussian (LoG) method, is often necessary to improve edge detection accuracy.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="laplacian-filter"><a class="header" href="#laplacian-filter">Laplacian Filter</a></h1>
<p>The Laplacian filter is a second-order derivative filter that detects areas of rapid intensity change. Unlike first-order methods (such as Sobel), which measure the gradient of intensity, the Laplacian measures the rate of change of the gradients, offering a different perspective on edge detection.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="derivative-theorem-of-convolution"><a class="header" href="#derivative-theorem-of-convolution">Derivative Theorem of Convolution</a></h1>
<p>The derivative theorem of convolution is a key concept in image processing. It states that the derivative of a convolution is the convolution of one function with the derivative of the other:</p>
<p>[
(f <em>g)' = f'</em> g = f * g'
]</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="laplacian-of-gaussian"><a class="header" href="#laplacian-of-gaussian">Laplacian of Gaussian</a></h1>
<p>The Laplacian of Gaussian (LoG) combines Gaussian smoothing with the Laplacian operator. It is effective for detecting edges by highlighting areas of rapid intensity change.</p>
<ol>
<li><strong>Gaussian Smoothing</strong>: First, a Gaussian filter is applied to reduce noise.</li>
<li><strong>Laplacian Operation</strong>: The Laplacian is applied to the smoothed image to detect areas of intensity change. The sign of the Laplacian helps identify whether an edge is a transition from light to dark or vice versa.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="zero-crossing"><a class="header" href="#zero-crossing">Zero Crossing</a></h1>
<p>A zero crossing occurs where the Laplacian changes sign, indicating potential edges. This typically corresponds to regions where image intensity exhibits a steep slope, making it a useful method for edge detection.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="edge-detection"><a class="header" href="#edge-detection">Edge Detection</a></h1>
<p>An "optimal edge detector" aims to accurately identify true edges while minimizing false detections and the effects of noise. According to John Canny's criteria (Canny, 1986), an optimal edge detector should have:</p>
<ul>
<li><strong>Good Detection</strong>: Detect as many true edges as possible.</li>
<li><strong>Good Localization</strong>: Ensure detected edges are close to the actual edges.</li>
<li><strong>Minimal Response</strong>: Avoid multiple responses to a single edge.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="canny-edge-detector"><a class="header" href="#canny-edge-detector">Canny Edge Detector</a></h1>
<p>The Canny edge detector is one of the most effective algorithms for edge detection, adhering to Canny's criteria. The steps of the Canny algorithm are:</p>
<ol>
<li><strong>Noise Reduction</strong>: The image is smoothed using a Gaussian filter to reduce noise, which prevents false edges.</li>
<li><strong>Gradient Calculation</strong>: Gradients are calculated, typically using the Sobel operator, to estimate horizontal ((G_x)) and vertical ((G_y)) derivatives.</li>
<li><strong>Non-Maximum Suppression</strong>: Potential edges are thinned by retaining local maxima in the gradient direction, ensuring that edges remain sharp.</li>
<li><strong>Double Thresholding</strong>: Two thresholds (low and high) are applied:
<ul>
<li><strong>Strong Edges</strong>: Pixels with gradient magnitudes above the high threshold are considered strong edges.</li>
<li><strong>Weak Edges</strong>: Pixels with gradient magnitudes between the thresholds are retained only if connected to strong edges.</li>
</ul>
</li>
<li><strong>Edge Tracking by Hysteresis</strong>: Starting from strong edges, weak edges connected to them are also marked as edges, ensuring edge continuity.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fourier-analysis"><a class="header" href="#fourier-analysis">Fourier Analysis</a></h1>
<p>Fourier analysis is a mathematical method used to decompose functions or signals into sinusoids of different frequencies.</p>
<p>Fourier analysis plays a crucial role in computer vision by providing methods to process, analyze, and manipulate images in the frequency domain. Some of the most important applications are:</p>
<ul>
<li><strong>Image Filtering</strong>: Fourier analysis allows for efficient high-pass, low-pass, and band-pass filtering, which can be used to remove noise, blur, or enhance image features.</li>
<li><strong>Image Compression</strong>: By transforming an image into its frequency components, redundant or less important frequencies can be removed, leading to effective compression without significant loss of quality.</li>
<li><strong>Feature Extraction</strong>: Fourier transforms can highlight periodic patterns and textures in an image, which are useful for object recognition and classification.</li>
<li><strong>Edge Detection</strong>: Edges in images can be detected by focusing on specific frequency ranges within the Fourier transform of an image.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fourier-series"><a class="header" href="#fourier-series">Fourier Series</a></h1>
<p>The Fourier series represents a periodic function as a sum of sine and cosine functions, or equivalently, as a sum of complex exponentials. A function (f(t)) that is periodic over a period (T) can be represented as:</p>
<p>[
f(t) = a_0 + \sum_{n=1}^\infty \left(a_n \cos\left(\frac{2\pi n t}{T}\right) + b_n \sin\left(\frac{2\pi n t}{T}\right)\right)
]</p>
<p>Where the Fourier coefficients (a_n) and (b_n) are given by:</p>
<p>[
a_n = \frac{2}{T} \int_0^T f(t) \cos\left(\frac{2\pi n t}{T}\right) , dt
]
[
b_n = \frac{2}{T} \int_0^T f(t) \sin\left(\frac{2\pi n t}{T}\right) , dt
]</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fourier-transform"><a class="header" href="#fourier-transform">Fourier Transform</a></h1>
<p>The Fourier transform extends the idea of Fourier series to non-periodic functions, providing a frequency spectrum for functions defined over all real numbers. The Fourier transform of a function (f(t)) is defined as:</p>
<p>[
\hat{f}(\omega) = \int_{-\infty}^{\infty} f(t) e^{-i \omega t} , dt
]</p>
<p>Where (\omega) is the angular frequency, and the integral is taken over the entire real line, capturing the function's values across time.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="discrete-fourier-transform"><a class="header" href="#discrete-fourier-transform">Discrete Fourier Transform</a></h1>
<p>In image processing, Fourier transforms are primarily used in their discrete form, known as the Discrete Fourier Transform (DFT). Since images do not naturally repeat across boundaries, they are treated as aperiodic signals, making the Fourier transform ideal for analyzing such non-periodic data.</p>
<p>The DFT converts an image to the frequency domain, facilitating operations such as filtering, enhancement, and noise reduction. For digital images, which consist of discrete pixel values, the DFT is used. The DFT is typically represented through its magnitude and phase components:</p>
<ul>
<li><strong>Magnitude Spectrum</strong>: Shows the amplitude of each frequency component in the signal, indicating how much of each frequency is present in the original signal.</li>
<li><strong>Phase Spectrum</strong>: Indicates the shift or displacement of each frequency component relative to the start of the signal.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="convolution-theorem"><a class="header" href="#convolution-theorem">Convolution Theorem</a></h1>
<p>The Convolution Theorem states that convolution in the time domain corresponds to multiplication in the frequency domain, and vice versa. This is particularly useful in signal and image processing, as convolution can be computationally expensive. Using the Fourier transform, convolution can be performed more efficiently:</p>
<ul>
<li>Apply the Fourier transform to both the signal and the filter.</li>
<li>Multiply their transforms in the frequency domain.</li>
<li>Apply the inverse Fourier transform to the result.</li>
</ul>
<p>This approach is much faster than directly performing convolution in the spatial domain, especially for large data sets.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="blurring-in-time-and-frequency-domains"><a class="header" href="#blurring-in-time-and-frequency-domains">Blurring in Time and Frequency Domains</a></h1>
<p>In the time domain, blurring is achieved by convolving an image with a blur kernel (a small matrix). The kernel is slid over the image, and at each position, the sum of the weighted pixel values covered by the kernel replaces the central pixel. The kernel's shape and size affect the blur characteristics. For example, a Gaussian kernel produces smooth gradients, while a square kernel tends to preserve edges, potentially introducing artifacts.</p>
<p>In the frequency domain, blurring is performed by multiplying the Fourier transform of the image by a filter (also in the frequency domain). A Gaussian filter in the frequency domain has a smooth decay, gradually attenuating high frequencies and preserving natural transitions. A square filter, however, has a sharp cutoff, eliminating high frequencies abruptly, which can introduce ringing artifacts (Gibbs phenomenon).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="filters-1"><a class="header" href="#filters-1">Filters</a></h1>
<ul>
<li><strong>Low Pass Filters (LPF)</strong>: Allow low frequencies to pass through while attenuating high frequencies. In images, low frequencies represent smooth intensity variations, which are associated with general features and areas of uniform color. LPFs smooth the image, reducing noise and detail.</li>
<li><strong>High Pass Filters (HPF)</strong>: Allow high frequencies to pass while attenuating low frequencies. High frequencies represent rapid intensity changes, such as edges. HPFs enhance or detect edges, making them useful for edge detection and sharpening.</li>
<li><strong>Band Pass Filters (BPF)</strong>: Allow a specific band of frequencies to pass while blocking frequencies outside this range. BPFs can isolate frequencies within a chosen range, useful for focusing on specific details or textures in the image.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nyquist-sampling-theorem"><a class="header" href="#nyquist-sampling-theorem">Nyquist Sampling Theorem</a></h1>
<p>The Nyquist Sampling Theorem states:</p>
<blockquote>
<p>To perfectly reconstruct a continuous signal from its samples, the sampling rate must be at least twice the highest frequency present in the signal (the Nyquist rate).</p>
</blockquote>
<p>When a signal is undersampled (i.e., sampled below the Nyquist rate), higher frequencies are misinterpreted as lower frequencies, leading to distortions known as aliasing.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="relation-with-gaussian-pyramids"><a class="header" href="#relation-with-gaussian-pyramids">Relation with Gaussian Pyramids</a></h1>
<p>A Gaussian Pyramid is created by repeatedly reducing an image's resolution, applying Gaussian blurring at each step. This process produces a stack of progressively lower-resolution images. Gaussian blurring ensures that high-frequency content is removed before subsampling, which prevents aliasing and adheres to the Nyquist criterion.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="relation-to-human-vision"><a class="header" href="#relation-to-human-vision">Relation to Human Vision</a></h1>
<p>The human eye contains photoreceptor cells (rods and cones) that sample the continuous visual field. These cells are densely packed in the fovea (central region) and sparse toward the periphery, allowing for high-resolution sampling where focus is directed, akin to a higher sampling rate at the fovea. Just as in digital sampling, improper sampling in the visual system can lead to aliasing, such as when viewing fine patterns or distant scenes.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hybrid-images"><a class="header" href="#hybrid-images">Hybrid Images</a></h1>
<p>Hybrid images combine two images using multi-resolution techniques. One image retains only high-frequency information (details), and the other retains only low-frequency information (general shape). These images are combined by simple addition:</p>
<ul>
<li>At close viewing distances, the human eye perceives the high-frequency details of the high-pass filtered image.</li>
<li>At farther distances, the eye naturally filters out finer details, leaving the broader, low-frequency features from the low-pass filtered image.</li>
</ul>
<p>This results in an image that changes interpretation based on the viewing distance.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pyramids"><a class="header" href="#pyramids">Pyramids</a></h1>
<p>In computer vision, pyramids are a concept used to represent an image at different scales, typically to enable multi-scale processing and analysis. There are mainly two types of pyramids: Gaussian pyramids and Laplacian pyramids. Both types are utilized in various applications, such as image compression, feature extraction, image blending, and more efficient and effective image analysis.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gaussian-pyramid"><a class="header" href="#gaussian-pyramid">Gaussian Pyramid</a></h1>
<p>A Gaussian pyramid is a series of progressively smaller images derived from the original image. Each subsequent image is smoothed using a Gaussian filter and then sub-sampled. This process is repeated multiple times, generating a stack of images where each level is a reduced-resolution version of the previous one. The steps involved are:</p>
<ol>
<li><strong>Apply Gaussian Blur</strong>: Smooth the image using a Gaussian filter.</li>
<li><strong>Sub-sample</strong>: Reduce the resolution of the image, typically by removing every alternate row and column.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="laplacian-pyramid"><a class="header" href="#laplacian-pyramid">Laplacian Pyramid</a></h1>
<p>A Laplacian pyramid is closely related to the Gaussian pyramid and is formed by taking the difference between consecutive images in the Gaussian pyramid. It captures the image detail lost between one level of the Gaussian pyramid and the next finer level. The process involves:</p>
<ol>
<li><strong>Create Gaussian Pyramid</strong>: First, create the Gaussian pyramid.</li>
<li><strong>Form Laplacian Layers</strong>: Each layer of the Laplacian pyramid is formed by subtracting the expanded version of the upper Gaussian layer from the corresponding Gaussian layer.</li>
</ol>
<p>The Laplacian pyramid is particularly useful in image compression and enhancement because it stores detailed information, making it easier to reconstruct finer details during the expansion process.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-reconstruction-with-laplacian-pyramid"><a class="header" href="#image-reconstruction-with-laplacian-pyramid">Image Reconstruction with Laplacian Pyramid</a></h1>
<p>The Laplacian pyramid allows for a perfect reconstruction of the original image. By storing the difference between each level of the Gaussian pyramid and its upsampled version, the Laplacian pyramid effectively tracks the exact data lost during each downsampling step. When reconstructing the image, these differences are added back, step by step, to the upsampled images, allowing for an exact reconstruction of the original image.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="panorama-stitching"><a class="header" href="#panorama-stitching">Panorama Stitching</a></h1>
<p>Panorama stitching is a crucial technique in computer vision, applied across many fields to solve practical and complex problems. Below are key motivations for its importance:</p>
<ul>
<li><strong>Wider Field of View</strong>: Panorama stitching enables the creation of images with a wider field of view than what can be captured by a single image. This is especially important in fields like real estate, where a comprehensive view of a property is needed, and in robotics, where a broader view assists in navigation and environmental understanding.</li>
<li><strong>Higher Resolution</strong>: By combining multiple images, panoramas achieve higher resolution and detail than individual images. This is vital for applications like digital mapping (e.g., Google Street View), where detailed, high-quality imagery is necessary for navigation and information extraction.</li>
<li><strong>Immersive Experiences</strong>: Panorama stitching is essential in creating immersive environments for Virtual Reality (VR) and Augmented Reality (AR). These technologies rely on seamless panoramic images to deliver convincing experiences without visual discontinuities.</li>
</ul>
<p>Panorama stitching enhances the capabilities of imaging devices by enabling broader and more detailed analysis, improving visual content creation, and addressing practical challenges in various industries that rely on computer vision.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="key-steps-in-panorama-stitching"><a class="header" href="#key-steps-in-panorama-stitching">Key Steps in Panorama Stitching</a></h1>
<ol>
<li>
<p><strong>Feature Detection</strong>:<br />
The first step in image stitching involves detecting distinctive features in each image. These features should be invariant to changes in scale, rotation, and illumination. Common algorithms include:</p>
<ul>
<li><strong>SIFT</strong> (Scale-Invariant Feature Transform)</li>
<li><strong>ORB</strong> (Oriented FAST and Rotated BRIEF)</li>
</ul>
<p>These algorithms identify keypoints such as corners, edges, or unique patterns that signify significant content changes.</p>
</li>
<li>
<p><strong>Feature Matching</strong>:<br />
After detecting keypoints, the next step is to find corresponding features between different images. This typically involves using feature descriptors—unique signatures that describe features invariant to transformations. Popular algorithms like SIFT and SURF compute descriptors for each keypoint, capturing the local appearance around each point and maintaining invariance to changes in scale, rotation, and lighting.</p>
</li>
<li>
<p><strong>Transform Model Estimation</strong>:<br />
Once features are matched between images, the geometric transformation that aligns one image with another is estimated. Depending on the camera motion, this transformation could be:</p>
<ul>
<li>Simple translation</li>
<li>Rotation</li>
<li>Complex models like affine or homography transformations</li>
</ul>
<p>Algorithms such as <strong>RANSAC</strong> (Random Sample Consensus) are commonly used to estimate the best transformation robustly by iteratively selecting a subset of matches, estimating the transformation, and verifying its alignment across all matches.</p>
</li>
<li>
<p><strong>Image Warping and Transformation</strong>:<br />
After determining the transformation model, it is applied to warp the images to align them. This involves adjusting the pixels of one or more images to ensure that corresponding features in the images match.</p>
</li>
<li>
<p><strong>Image Blending</strong>:<br />
Finally, the aligned images must be blended together to create a seamless panorama. Blending techniques manage overlaps, ensure color consistency, and smooth transitions between images. Common methods include:</p>
<ul>
<li><strong>Multi-band blending</strong>: Blends images at different scales for better visual quality.</li>
<li><strong>Alpha blending</strong>: Facilitates gradual transitions between images to avoid abrupt changes.</li>
</ul>
</li>
</ol>
<p>By following these steps, a high-quality panoramic image is produced, enhancing the final result in terms of both visual quality and practical application.
multi-band blending, which blends images at different scales, or alpha blending, which facilitates gradual transitions between images, can be used to enhance the visual quality of the panorama.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="features-in-computer-vision"><a class="header" href="#features-in-computer-vision">Features in Computer Vision</a></h1>
<p>In computer vision, "features" refer to specific patterns or unique attributes within an image that are important for analyzing and understanding its content. These features vary depending on the task but generally include elements that help differentiate one part of an image from another.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="common-types-of-features"><a class="header" href="#common-types-of-features">Common Types of Features</a></h1>
<ul>
<li>
<p><strong>Edges</strong>:<br />
Edges represent boundaries where sharp changes in brightness occur. They are crucial for detecting objects, boundaries, and shapes within an image.</p>
</li>
<li>
<p><strong>Corners and Interest Points</strong>:<br />
Corners are points where two or more edges meet. They are often used because they are invariant to translation, rotation, and changes in illumination. Interest points are distinct, recognizable pixels within the image that can be matched across different images of the same scene or object.</p>
</li>
<li>
<p><strong>Blobs and Regions</strong>:<br />
Blobs are regions that differ in brightness, color, or other properties compared to surrounding areas. These are useful for detecting objects that are cohesive in terms of color or intensity.</p>
</li>
<li>
<p><strong>Textures</strong>:<br />
Texture refers to the surface variations in intensity that suggest properties like material or surface quality. Texture features are valuable in classifying materials or objects based on their surface characteristics.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="characteristics-of-a-good-feature"><a class="header" href="#characteristics-of-a-good-feature">Characteristics of a Good Feature</a></h1>
<p>A good feature in computer vision effectively contributes to tasks such as classification, matching, tracking, or reconstruction. Key characteristics of a good feature include:</p>
<ul>
<li>
<p><strong>Distinctiveness</strong>:<br />
A good feature should provide enough information to distinguish between different objects or classes, while being robust to irrelevant variations.</p>
</li>
<li>
<p><strong>Invariance</strong>:<br />
Features need to be invariant to certain transformations, depending on the application. Common invariances include:</p>
<ul>
<li><strong>Scale</strong>: The feature should be detectable in both small and large sizes.</li>
<li><strong>Rotation</strong>: The feature should be recognizable regardless of its orientation.</li>
<li><strong>Illumination</strong>: Changes in lighting should not affect feature detectability.</li>
<li><strong>Viewpoint</strong>: The feature should ideally be recognizable from different angles, especially in 3D applications.</li>
</ul>
</li>
<li>
<p><strong>Repeatability</strong>:<br />
The feature should be detectable under varying conditions. If identified in one image, it should be recognizable in another image where the scene appears under different conditions.</p>
</li>
<li>
<p><strong>Efficiency</strong>:<br />
For real-time applications, feature extraction and matching must be computationally efficient, ensuring that the process is not prohibitively slow.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="local-vs-global-features"><a class="header" href="#local-vs-global-features">Local vs. Global Features</a></h1>
<p>Features extracted from images can be broadly categorized into:</p>
<ul>
<li>
<p><strong>Local Features</strong>:<br />
Focus on specific points or regions within the image, often used for tasks like matching or object detection.</p>
</li>
<li>
<p><strong>Global Features</strong>:<br />
Capture broader, overall characteristics of the image and are typically used for tasks such as scene recognition or image classification.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="feature-detectors"><a class="header" href="#feature-detectors">Feature Detectors</a></h1>
<p>Feature detectors are algorithms designed to identify points of interest in an image. These points, or "features," are typically locations where the image content changes significantly, such as edges, corners, or blobs. The primary goal of a feature detector is to locate salient points that are distinctive and invariant to transformations like translation, rotation, or scaling.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="global-feature-detectors"><a class="header" href="#global-feature-detectors">Global Feature Detectors</a></h1>
<p>Global features describe characteristics of the entire image, such as its shape, texture, or color histogram. They summarize the image content as a whole rather than focusing on specific areas, making them ideal for:</p>
<ul>
<li><strong>Image classification</strong>: The whole image defines a category, such as distinguishing between different types of landscapes.</li>
<li><strong>Scene recognition</strong>: The overall context or setting of the image is more important than individual elements within it.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="local-feature-detectors"><a class="header" href="#local-feature-detectors">Local Feature Detectors</a></h1>
<p>Local features capture information about specific points or small regions in the image. These features are particularly useful for tasks that require precise structure or content information from parts of the image. Applications include:</p>
<ul>
<li><strong>Object recognition</strong>: Useful for recognizing objects that appear at different sizes, rotations, or with partial occlusion.</li>
<li><strong>Image matching</strong>: Align or stitch images by identifying similar patterns or keypoints across images.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="harris-corner-detector"><a class="header" href="#harris-corner-detector">Harris Corner Detector</a></h1>
<p>The <strong>Harris Corner Detector</strong> is a well-known method for detecting corners, regions where there is a significant change in intensity in multiple directions. Corners are points where two or more edges meet, and detecting them is essential for tasks such as image matching and object recognition.</p>
<p>The algorithm is based on the idea that corners can be detected by analyzing how the image brightness changes when shifted slightly in different directions. This can be quantified using the <strong>summed square difference (SSD)</strong> function, which compares image patches before and after a shift. The intensity change is large around corners but small around edges or flat regions.</p>
<p>The SSD function is defined as:</p>
<p>$$
E_{\text{SSD}}(u) = \sum_{i} [I_1(x_i + u) - I_0(x_i)]^2
$$</p>
<p>Where:</p>
<ul>
<li>( u ) is the small shift vector.</li>
<li>( I_1(x_i + u) ) is the intensity at location ( x_i + u ).</li>
<li>( I_0(x_i) ) is the original intensity at location ( x_i ).</li>
</ul>
<p>We can enhance this with a spatial weighting function:</p>
<p>$$
E_{\text{wSSD}}(u) = \sum_i w(x_i) [I_1(x_i + u) - I_0(x_i)]^2
$$</p>
<p>Here, ( w(x_i) ) is a Gaussian windowing function that emphasizes central pixels in the image patch.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="small-motion-assumption"><a class="header" href="#small-motion-assumption">Small Motion Assumption</a></h1>
<p>Under the small motion assumption, the intensity change due to a small shift ( \Delta u ) is approximated linearly using a Taylor expansion:</p>
<p>$$
I(x_i + \Delta u) \approx I(x_i) + \nabla I(x_i) \cdot \Delta u
$$</p>
<p>Substituting into the SSD gives us:</p>
<p>$$
wSSD(\Delta u) = \sum_{i} w(x_i) [(\nabla I(x_i) \cdot \Delta u)^2]
$$</p>
<p>This can be further expanded:</p>
<p>$$
wSSD(\Delta u) = \sum_{i} w(x_i) [(I_x(x_i) u_x + I_y(x_i) u_y)^2]
$$</p>
<p>This leads to the quadratic form:</p>
<p>$$
wSSD(\Delta u) = \sum_{i} w(x_i) \begin{bmatrix} u_x &amp; u_y \end{bmatrix} \begin{bmatrix} I_x^2 &amp; I_x I_y \ I_x I_y &amp; I_y^2 \end{bmatrix} \begin{bmatrix} u_x \ u_y \end{bmatrix}
$$</p>
<p>Defining the matrix ( \mathbf{A} ) as:</p>
<p>$$
\mathbf{A} = \sum_{i} w(x_i) \begin{bmatrix} I_x^2 &amp; I_x I_y \ I_x I_y &amp; I_y^2 \end{bmatrix}
$$</p>
<p>Matrix ( A ), also known as the <strong>structure tensor</strong>, captures the local image gradients, which help in determining whether a region is a corner, edge, or flat area. The eigenvalues of ( A ) describe the local image structure:</p>
<ul>
<li>Large, comparable eigenvalues indicate a corner.</li>
<li>One large eigenvalue suggests an edge.</li>
<li>Small eigenvalues correspond to flat regions.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="harris-corner-detector-algorithm"><a class="header" href="#harris-corner-detector-algorithm">Harris Corner Detector Algorithm</a></h1>
<ol>
<li>
<p>Compute image gradients ( I_x ) and ( I_y ) (often using a Sobel filter), and smooth them with a Gaussian filter:
$$
S_{xx} = G * I_x^2, \quad S_{yy} = G * I_y^2, \quad S_{xy} = G * I_x I_y
$$</p>
</li>
<li>
<p>Construct the second moment matrix ( A ) using the smoothed gradients:
$$
\mathbf{A} = \begin{bmatrix} S_{xx} &amp; S_{xy} \ S_{xy} &amp; S_{yy} \end{bmatrix}
$$</p>
</li>
<li>
<p>Compute the <strong>corner response function</strong>:
$$
R = \det(\mathbf{A}) - k \cdot (\text{trace}(\mathbf{A}))^2
$$
Where:
$$
\det(\mathbf{A}) = S_{xx} S_{yy} - S_{xy}^2, \quad \text{trace}(\mathbf{A}) = S_{xx} + S_{yy}
$$</p>
</li>
<li>
<p>Apply a threshold to ( R ) to detect potential corners and use <strong>non-maximum suppression</strong> to keep only local maxima.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="properties-of-the-harris-corner-detector"><a class="header" href="#properties-of-the-harris-corner-detector">Properties of the Harris Corner Detector</a></h1>
<ul>
<li><strong>Locality</strong>: Sensitive to local features.</li>
<li><strong>Illumination Invariance</strong>: Robust to changes in lighting.</li>
<li><strong>Rotation Invariance</strong>: Corners are detected irrespective of image orientation.</li>
<li><strong>Partial Scale Invariance</strong>: Can be extended to detect features across scales using a multi-scale approach.</li>
<li><strong>Sensitivity to Perspective</strong>: Less robust to perspective transformations without modifications.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="harris-corner-detector-with-scale-invariance"><a class="header" href="#harris-corner-detector-with-scale-invariance">Harris Corner Detector with Scale-Invariance</a></h1>
<p>To make the Harris Corner Detector <strong>scale-invariant</strong>, it can be combined with the <strong>Laplacian of Gaussian (LoG)</strong> filter. This allows detection of features at multiple scales:</p>
<ol>
<li>Apply a Gaussian filter with varying ( \sigma ) values.</li>
<li>Compute image gradients and the second moment matrix at each scale.</li>
<li>Compute the Harris response at each scale.</li>
<li>Perform non-maximum suppression across both spatial and scale dimensions to ensure that detected corners are local maxima.</li>
</ol>
<p>This approach ensures corners are detected robustly across different scales, adapting to the intrinsic size of the features.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="blob-detector"><a class="header" href="#blob-detector">Blob Detector</a></h1>
<p>A <strong>blob</strong> refers to a region in the image that differs in properties, such as brightness or color, from surrounding areas. The <strong>Laplacian of Gaussian (LoG)</strong> is often used as a blob detector:</p>
<ol>
<li>Apply the Gaussian filter to smooth the image.</li>
<li>Use the Laplacian filter to detect regions of rapid intensity change (blobs).</li>
<li>Identify <strong>zero crossings</strong> in the Laplacian response, which indicate blob boundaries.</li>
</ol>
<p>For <strong>scale-invariance</strong>, this process can be applied using a <strong>Gaussian pyramid</strong>:</p>
<ul>
<li>At each level of the pyramid, apply the LoG filter to detect blobs at different scales.</li>
<li>Significant blobs are those that appear consistently across scales.</li>
</ul>
<p>This method ensures that blob-like features are detected at appropriate scales, making it a powerful tool for detecting regions of interest in an image.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="feature-descriptors"><a class="header" href="#feature-descriptors">Feature Descriptors</a></h1>
<p>Feature descriptors are numerical representations of image characteristics, typically extracted from specific regions or points of interest. While feature detectors identify significant points such as corners, edges, or blobs, feature descriptors provide a compact, robust, and informative description of these points. Descriptors aim to be invariant to variations in illumination, rotation, and scale, making them crucial for comparing and matching features across different images.</p>
<p>Once features have been detected, descriptors encapsulate information about the appearance and shape of the surrounding area to allow robust matching across images.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="building-histogram-of-oriented-gradients-hog"><a class="header" href="#building-histogram-of-oriented-gradients-hog">Building Histogram of Oriented Gradients (HoG)</a></h1>
<p>HoG (Histogram of Oriented Gradients) is a feature descriptor that describes the shape and appearance of objects by analyzing the distribution of gradient orientations. Below is the step-by-step construction of HoG, starting from simple approaches and progressively adding enhancements.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-patches"><a class="header" href="#image-patches">Image Patches</a></h1>
<ul>
<li><strong>Basic Approach</strong>: Use raw pixel intensity values inside an image patch.</li>
<li><strong>Limitation</strong>: Sensitive to absolute intensity and fails under changes in illumination or geometric transformations like rotation or scaling.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-gradients"><a class="header" href="#image-gradients">Image Gradients</a></h1>
<ul>
<li><strong>Improvement</strong>: Use image gradients (changes in intensity) instead of raw intensity. Gradients are more robust against illumination variations.</li>
<li><strong>Limitation</strong>: Gradients improve robustness but do not handle geometric transformations like rotation and scale.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="color-histograms"><a class="header" href="#color-histograms">Color Histograms</a></h1>
<ul>
<li><strong>Improvement</strong>: Color histograms summarize the distribution of colors, providing robustness against scaling and rotation.</li>
<li><strong>Limitation</strong>: Color histograms do not capture spatial information, which can lead to poor matching when spatial arrangement is crucial.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="spatial-histograms"><a class="header" href="#spatial-histograms">Spatial Histograms</a></h1>
<ul>
<li><strong>Improvement</strong>: Divide the image into cells and compute histograms for each cell, capturing spatial layout. This retains both appearance and spatial information, often based on gradients rather than color.</li>
<li><strong>Limitation</strong>: Still not fully invariant to rotation.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="orientation-normalization"><a class="header" href="#orientation-normalization">Orientation Normalization</a></h1>
<ul>
<li><strong>Improvement</strong>: Align histograms according to a dominant orientation to achieve rotation invariance.</li>
<li><strong>Limitation</strong>: Orientation normalization does not address challenges like object identity based solely on color distribution.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hog-algorithm"><a class="header" href="#hog-algorithm">HoG Algorithm</a></h1>
<p>HoG combines gradient, spatial structuring, and orientation normalization into a coherent descriptor. The steps to compute HoG are:</p>
<ol>
<li><strong>Preprocessing</strong>: Convert the image to grayscale to reduce complexity and focus on structure rather than color.</li>
<li><strong>Gradient Computation</strong>: Compute the horizontal and vertical gradients for each pixel, often using a Sobel filter.</li>
<li><strong>Orientation Binning</strong>: Divide the image into cells (e.g., 8x8 pixels). For each cell, create a histogram of gradient orientations, typically with 9 to 18 bins covering 0 to 180 degrees (unsigned) or 0 to 360 degrees (signed).</li>
<li><strong>Descriptor Blocks</strong>: Group adjacent cells into larger blocks (e.g., 2x2 cells). Normalize histograms within each block to reduce sensitivity to lighting variations.</li>
<li><strong>Concatenation</strong>: Combine the normalized histograms from all blocks into a single feature vector representing the HoG descriptor.</li>
<li><strong>Sliding Window</strong>: For object detection, apply HoG within a sliding window across the image.</li>
</ol>
<p><strong>HoG Applications</strong>: HoG is particularly effective in human detection, capturing vertical and horizontal edges typical in human forms.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scale-invariant-feature-transform-sift"><a class="header" href="#scale-invariant-feature-transform-sift">Scale-Invariant Feature Transform (SIFT)</a></h1>
<p>SIFT is both a feature detector and descriptor, designed to be invariant to scale and rotation while robust against illumination changes. It is widely used for tasks like object recognition and image matching.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sift-algorithm"><a class="header" href="#sift-algorithm">SIFT Algorithm</a></h1>
<ol>
<li><strong>Scale-Space Construction</strong>: Construct a scale space by applying Gaussian filters at different scales. The Difference of Gaussians (DoG) is used to detect keypoints.</li>
<li><strong>Keypoint Localization</strong>: Find local maxima and minima in the DoG. Keypoints are refined using Taylor expansion to achieve sub-pixel accuracy.</li>
<li><strong>Orientation Assignment</strong>: Compute the gradient magnitude and orientation around each keypoint. The orientation histogram is created, and the keypoint is assigned a dominant orientation.</li>
<li><strong>Descriptor Computation</strong>:
<ul>
<li>Calculate gradients in a 16x16 region around the keypoint.</li>
<li>Rotate the region according to the keypoint’s orientation.</li>
<li>Divide the region into 4x4 cells and compute orientation histograms for each.</li>
<li>Concatenate the histograms into a 128-element vector.</li>
</ul>
</li>
</ol>
<p><strong>SIFT Properties</strong>:</p>
<ul>
<li><strong>Scale and Rotation Invariance</strong>: SIFT features are invariant to changes in scale and orientation.</li>
<li><strong>Robustness to Illumination</strong>: The use of gradients makes SIFT somewhat invariant to lighting changes.</li>
<li><strong>Distinctiveness</strong>: The 128-element descriptor captures significant local structure, making it highly distinctive.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="speeded-up-robust-features-surf"><a class="header" href="#speeded-up-robust-features-surf">Speeded Up Robust Features (SURF)</a></h1>
<p>SURF is a faster alternative to SIFT, designed for speed and efficiency while maintaining robustness. It uses integral images and approximates Gaussian convolutions with box filters for quick computation.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="surf-algorithm"><a class="header" href="#surf-algorithm">SURF Algorithm</a></h1>
<ol>
<li><strong>Integral Image</strong>: Compute the integral image for fast calculation of box-type filters, which approximate Gaussian filters.</li>
<li><strong>Hessian Matrix</strong>: Use the Hessian matrix to detect keypoints. The determinant of the Hessian matrix highlights blob-like structures in the image.</li>
<li><strong>Keypoint Localization</strong>: Identify local maxima and minima in the determinant of the Hessian matrix across scales.</li>
<li><strong>Descriptor Construction</strong>:
<ul>
<li>Assign orientation based on Haar wavelet responses.</li>
<li>Divide the region around the keypoint into 4x4 subregions.</li>
<li>Compute Haar wavelet responses in each subregion to capture gradient information.</li>
<li>Normalize the descriptor to achieve robustness against lighting changes.</li>
</ul>
</li>
</ol>
<p><strong>SURF Properties</strong>:</p>
<ul>
<li><strong>Speed</strong>: Faster than SIFT due to the use of integral images and box filters.</li>
<li><strong>Robustness</strong>: Maintains robustness to scale, rotation, and illumination changes.</li>
<li><strong>Efficient Matching</strong>: Laplacian indexing enhances matching efficiency by considering the polarity of keypoints.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="binary-descriptors"><a class="header" href="#binary-descriptors">Binary Descriptors</a></h1>
<p>Binary descriptors are lightweight and computationally efficient. Instead of using floating-point vectors like SIFT or SURF, binary descriptors create compact binary strings for faster matching.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="brief-binary-robust-independent-elementary-features"><a class="header" href="#brief-binary-robust-independent-elementary-features">BRIEF (Binary Robust Independent Elementary Features)</a></h1>
<ul>
<li><strong>Approach</strong>: Generate a binary string by comparing intensities of pixel pairs in an image patch.</li>
<li><strong>Limitation</strong>: Not rotation invariant and sensitive to noise.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="orb-oriented-fast-and-rotated-brief"><a class="header" href="#orb-oriented-fast-and-rotated-brief">ORB (Oriented FAST and Rotated BRIEF)</a></h1>
<ul>
<li><strong>Approach</strong>: Combines the FAST keypoint detector with BRIEF descriptors, adding orientation information for rotation invariance.</li>
<li><strong>Efficiency</strong>: ORB is fast and free to use, making it suitable for real-time applications.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="freak-fast-retina-keypoint"><a class="header" href="#freak-fast-retina-keypoint">FREAK (Fast Retina Keypoint)</a></h1>
<ul>
<li><strong>Approach</strong>: Inspired by the human visual system, FREAK compares pixel intensities across a retinal sampling pattern. The pattern is denser near the center and sparser towards the periphery, mimicking retinal ganglion cells.</li>
<li><strong>Advantage</strong>: More robust to rotation and scale changes compared to BRIEF, with a lower computational cost than SIFT or SURF.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="feature-matching"><a class="header" href="#feature-matching">Feature Matching</a></h1>
<p>Feature matching is the process of identifying corresponding keypoints between different images. It is a critical step in various computer vision applications, such as stereo vision, object recognition, motion tracking, image stitching, and 3D reconstruction. The goal of feature matching is to establish correspondences between features extracted from different images, even when those images are captured under different conditions like varying scale, orientation, or lighting.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="key-steps-in-feature-matching"><a class="header" href="#key-steps-in-feature-matching">Key Steps in Feature Matching</a></h1>
<ol>
<li>
<p><strong>Feature Detection</strong>:<br />
Features or keypoints are first detected in each image. These points represent areas of the image with significant texture or structure (e.g., corners or edges), which can be reliably identified and are typically invariant to transformations like scaling, rotation, or changes in illumination.</p>
</li>
<li>
<p><strong>Feature Description</strong>:<br />
Each detected feature is described using a feature descriptor. Descriptors encode the appearance of the feature and its surrounding region in a compact form, making them robust against transformations. The goal is for the same feature to have a similar descriptor even when captured under different conditions.</p>
</li>
<li>
<p><strong>Feature Matching</strong>:<br />
Once features are described, the descriptors are matched across different images. This involves finding pairs of descriptors that are closest in terms of a chosen distance metric. Common metrics include:</p>
<ul>
<li><strong>Euclidean distance</strong>: Used for real-valued descriptors like SIFT and SURF. It calculates the straight-line distance between two points in a high-dimensional space.</li>
<li><strong>Hamming distance</strong>: Used for binary descriptors like ORB and BRIEF. It counts the number of differing bits between two binary strings, making it computationally efficient.</li>
</ul>
</li>
</ol>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h1 id="matching-techniques"><a class="header" href="#matching-techniques">Matching Techniques</a></h1>
<ul>
<li>
<p><strong>Brute-Force Matching</strong>:<br />
The simplest method of feature matching, where each descriptor from one image is compared with every descriptor in the second image. While exhaustive and accurate, brute-force matching can be computationally expensive for large datasets.</p>
</li>
<li>
<p><strong>Nearest Neighbor Matching</strong>:<br />
In this method, each descriptor from one image is matched with the descriptor from the second image that has the smallest distance. To improve robustness, techniques such as the ratio test (proposed by Lowe in SIFT) can be applied:</p>
<ul>
<li><strong>Ratio Test</strong>: Compares the distance of the nearest neighbor to the second nearest neighbor. If the ratio of the two distances is below a threshold, the match is accepted; otherwise, it is discarded to avoid false matches.</li>
</ul>
</li>
<li>
<p><strong>K-Nearest Neighbors (k-NN)</strong>:<br />
Instead of finding only the closest match, this method identifies the k closest matches for each feature. The ratio test can also be applied here to filter out ambiguous matches.</p>
</li>
<li>
<p><strong>FLANN (Fast Library for Approximate Nearest Neighbors)</strong>:<br />
This is a more efficient alternative to brute-force matching, especially for large datasets. FLANN uses approximate nearest neighbor search algorithms, which trade off a slight loss in accuracy for a significant speed improvement.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="applications-of-feature-matching"><a class="header" href="#applications-of-feature-matching">Applications of Feature Matching</a></h1>
<ul>
<li>
<p><strong>Image Stitching</strong>:<br />
In panorama creation, feature matching aligns overlapping images by finding corresponding points, allowing the images to be stitched together seamlessly.</p>
</li>
<li>
<p><strong>Object Recognition</strong>:<br />
Feature matching allows for recognizing objects across different images, even when they are rotated or viewed from different angles.</p>
</li>
<li>
<p><strong>3D Reconstruction</strong>:<br />
By matching keypoints between multiple views of the same scene, depth information can be estimated, enabling the reconstruction of 3D models from 2D images.</p>
</li>
<li>
<p><strong>Motion Tracking</strong>:<br />
In video analysis, feature matching helps track objects or features across consecutive frames, facilitating motion tracking or optical flow estimation.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deep-learning-approaches"><a class="header" href="#deep-learning-approaches">Deep Learning Approaches</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="superpoint-and-superglue"><a class="header" href="#superpoint-and-superglue">SuperPoint and SuperGlue</a></h1>
<p><strong>SuperPoint</strong> is a convolutional neural network (CNN) that simultaneously detects interest points and computes descriptors. It is pre-trained on synthetic data and fine-tuned on real-world images using self-supervised learning.</p>
<ul>
<li><strong>Dual Head Architecture</strong>: A shared CNN encoder extracts feature maps for both interest point detection and descriptor computation. One head produces a heatmap for keypoint detection, and the other outputs a dense descriptor map for describing keypoints.</li>
<li><strong>Keypoint Detection</strong>: Local maxima in the heatmap identify keypoints, with non-maximum suppression ensuring they are well-distributed.</li>
<li><strong>Descriptor Generation</strong>: Descriptors, extracted from the corresponding keypoints in the descriptor map, are designed to be distinctive and robust against image transformations.</li>
</ul>
<p><strong>SuperGlue</strong> is a feature matching method that enhances SuperPoint's output using a Graph Neural Network (GNN) to improve matching accuracy, especially in challenging conditions (e.g., large viewpoint changes or occlusions).</p>
<ul>
<li><strong>Graph Construction</strong>: Descriptors from SuperPoint (or another detector) are treated as graph nodes, with edges representing potential matches between two images.</li>
<li><strong>Graph Neural Network</strong>: The GNN refines matches by considering both local descriptor similarities and the global geometric consistency of features across the graph.</li>
<li><strong>Match Filtering</strong>: SuperGlue selects matches that are both locally and globally consistent, outperforming traditional nearest neighbor methods in complex scenarios.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-transformations"><a class="header" href="#image-transformations">Image Transformations</a></h1>
<p>In computer vision, a transformation modifies the geometry of an image, including its position, size, orientation, or perspective. A transformation maps points in one image to corresponding points in another, altering their spatial relationships.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="homogeneous-coordinates"><a class="header" href="#homogeneous-coordinates">Homogeneous Coordinates</a></h1>
<p>Homogeneous coordinates extend the representation of points by adding an extra dimension, allowing for more complex transformations, such as perspective changes. They also enable affine transformations, including translations, to be represented as matrix operations. Points at infinity, which are useful in perspective transformations, are represented by setting ( w = 0 ).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="linear-transformations"><a class="header" href="#linear-transformations">Linear Transformations</a></h1>
<p>Linear transformations include operations like translation, rotation, scaling, and shearing. These transformations are simple to compute and can be represented as matrix multiplications.</p>
<p>These include:</p>
<ul>
<li><strong>Rotation</strong>: Changes the orientation of an image.</li>
<li><strong>Scaling</strong>: Alters the size of an image.</li>
<li><strong>Shearing</strong>: Skews the image along one axis.</li>
</ul>
<p>They preserve linear relationships and distances between points and are expressed as matrix multiplications.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="affine-transformations"><a class="header" href="#affine-transformations">Affine Transformations</a></h1>
<p>Affine transformations include all linear transformations plus translation. They maintain parallelism of lines and can be expressed using homogeneous coordinates in matrix form:</p>
<p>$$
\begin{bmatrix}
x' \
y' \
1
\end{bmatrix} =
\begin{bmatrix}
a &amp; b &amp; t_x \
c &amp; d &amp; t_y \
0 &amp; 0 &amp; 1
\end{bmatrix}
\begin{bmatrix}
x \
y \
1
\end{bmatrix}
$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="homographies"><a class="header" href="#homographies">Homographies</a></h1>
<p>Homographies (projective transformations) extend affine transformations by allowing changes in perspective. They can warp, tilt, and scale image content and are particularly useful for stitching images with different viewpoints. Homographies are represented by a 3x3 matrix that transforms points in homogeneous coordinates:</p>
<p>$$
\begin{bmatrix}
x' \
y' \
w'
\end{bmatrix}=
\begin{bmatrix}
h_{11} &amp; h_{12} &amp; h_{13} \
h_{21} &amp; h_{22} &amp; h_{23} \
h_{31} &amp; h_{32} &amp; h_{33}
\end{bmatrix}
\begin{bmatrix}
x \
y \
1
\end{bmatrix}
$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="computing-transformations"><a class="header" href="#computing-transformations">Computing Transformations</a></h1>
<p>Computing a transformation involves determining the transformation matrix ( T ) that maps points from image ( A ) to image ( B ), given a set of matched feature points between the two images. The goal is to find the matrix that best fits the transformation using methods like least squares, which minimizes the error from imperfect feature matching.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="computing-translation"><a class="header" href="#computing-translation">Computing Translation</a></h1>
<p>Given matched points ( (x_i, y_i) ) in ( A ) and ( (x'_i, y'_i) ) in ( B ), the translation equations are:</p>
<p>$$
x'_i = x_i + t_x \
y'_i = y_i + t_y
$$</p>
<p>Rearranging gives:</p>
<p>$$
t_x = x'_i - x_i \
t_y = y'_i - y_i
$$</p>
<p>To compute ( t_x ) and ( t_y ) across all matches, the least squares approach minimizes the sum of squared differences:</p>
<p>$$
\min_{t_x, t_y} \sum_{i=1}^n \left((x'_i - x_i - t_x)^2 + (y'_i - y_i - t_y)^2\right)
$$</p>
<p>The solution for ( t_x ) and ( t_y ) is:</p>
<p>$$
\hat{t}<em>x = \frac{1}{n} \sum</em>{i=1}^n (x'_i - x_i), \quad \hat{t}<em>y = \frac{1}{n} \sum</em>{i=1}^n (y'_i - y_i)
$$</p>
<p>This averages the translation estimates from all matches, reducing the impact of noise.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="computing-affine-transformations"><a class="header" href="#computing-affine-transformations">Computing Affine Transformations</a></h1>
<p>For affine transformations, which include translation, rotation, scaling, and shearing, the relationship between matched points is:</p>
<p>$$
x'_i = a \cdot x_i + b \cdot y_i + t_x \
y'_i = c \cdot x_i + d \cdot y_i + t_y
$$</p>
<p>This introduces six unknowns ( a, b, c, d, t_x, t_y ). The least squares method minimizes:</p>
<p>$$
\min_{a, b, c, d, t_x, t_y} \sum_{i=1}^n \left((x'_i - (a \cdot x_i + b \cdot y_i + t_x))^2 + (y'_i - (c \cdot x_i + d \cdot y_i + t_y))^2\right)
$$</p>
<p>At least three matched points are required to compute the affine transformation.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="computing-homographies"><a class="header" href="#computing-homographies">Computing Homographies</a></h1>
<p>Homographies allow for perspective transformations, represented by a 3x3 matrix. The general form is:</p>
<p>$$
\begin{bmatrix}
x' \
y' \
w'
\end{bmatrix} =
\begin{bmatrix}
h_{11} &amp; h_{12} &amp; h_{13} \
h_{21} &amp; h_{22} &amp; h_{23} \
h_{31} &amp; h_{32} &amp; h_{33}
\end{bmatrix}
\begin{bmatrix}
x \
y \
1
\end{bmatrix}
$$</p>
<p>The non-linear relationship between input and output coordinates is linearized by cross-multiplying, leading to:</p>
<p>$$
x'(h_{31}x + h_{32}y + h_{33}) = h_{11}x + h_{12}y + h_{13} \
y'(h_{31}x + h_{32}y + h_{33}) = h_{21}x + h_{22}y + h_{23}
$$</p>
<p>Each matched point provides two equations, requiring at least four point pairs to solve for the eight unknowns (assuming ( h_{33} = 1 ) to resolve scale ambiguity).</p>
<p>To minimize error, least squares finds:</p>
<p>$$
|Ah - b|^2
$$</p>
<p>Where ( A ) is constructed from the linearized equations, and ( h ) is the homography vector. Singular Value Decomposition (SVD) is used to solve this overconstrained system, where the optimal ( h ) is the eigenvector of ( A^T A ) corresponding to the smallest eigenvalue, minimizing the residual error.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ransac"><a class="header" href="#ransac">RANSAC</a></h1>
<p>When computing transformations between two images, such as a homography between image ( A ) and image ( B ), feature matches are crucial. However, due to detection errors, scene changes, and mismatches, outliers can occur—pairs of points that do not correspond to the same physical point in space. Outliers can severely distort the computed transformation, leading to inaccurate results when using least squares optimization.</p>
<p><strong>RANSAC (Random Sample Consensus)</strong> is a robust algorithm used to estimate the transformation while mitigating the effects of outliers. RANSAC iteratively tries to find a model that best fits the inliers, ignoring the outliers. The steps are:</p>
<ol>
<li><strong>Sample Minimum Points</strong>: Randomly select the minimal number of point pairs required to estimate the transformation (e.g., 4 pairs for homography).</li>
<li><strong>Fit Model</strong>: Compute the transformation (e.g., homography) from the selected points.</li>
<li><strong>Evaluate Inliers</strong>: Apply the model to all matches and count the inliers—points that conform to the model within a defined error threshold.</li>
<li><strong>Iterate</strong>: Repeat the process, each time with different random point pairs, to find the transformation with the highest number of inliers.</li>
<li><strong>Select Optimal Model</strong>: Choose the model with the most inliers.</li>
<li><strong>Refine Model</strong>: Recalculate the transformation using only the inliers to improve accuracy.</li>
</ol>
<p>The number of RANSAC iterations ( N ) needed to achieve a high probability ( p ) of success is determined by:</p>
<p>$$
N = \frac{\log(1 - p)}{\log(1 - (1 - e)^s)}
$$</p>
<p>Where:</p>
<ul>
<li>( s ) is the number of points needed for the model (e.g., 4 for a homography),</li>
<li>( e ) is the proportion of outliers,</li>
<li>( p ) is the desired probability of finding a valid model.</li>
</ul>
<hr />
<p><strong>Pros of RANSAC</strong>:</p>
<ul>
<li><strong>Robustness</strong>: Effectively handles datasets with a high proportion of outliers.</li>
<li><strong>Generality</strong>: Can be applied to various model-fitting tasks beyond homographies, such as fundamental matrices and affine transformations.</li>
</ul>
<p><strong>Cons of RANSAC</strong>:</p>
<ul>
<li><strong>Computational Cost</strong>: High iteration count may be required when the proportion of outliers is large, leading to slower computations.</li>
<li><strong>Failure in Dense Outliers</strong>: If outliers dominate the dataset, RANSAC may struggle to find a valid model.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="warping"><a class="header" href="#warping">Warping</a></h1>
<p>After computing the homography between two images, warping is the process of applying this transformation to align or stitch the images. Warping modifies the coordinates of pixels to fit a new coordinate system. There are two main types:</p>
<ul>
<li>
<p><strong>Forward Warping</strong>: Directly applies the homography to each pixel in the source image, mapping it to a new position in the destination image. This can lead to issues like gaps (where no pixels are mapped) or overlaps (where multiple pixels map to the same destination). Post-processing is required to fill gaps or resolve overlaps, potentially affecting image quality.</p>
</li>
<li>
<p><strong>Inverse Warping</strong>: Iterates over each pixel in the destination image and uses the inverse of the homography to map back to the source image. This method avoids gaps by ensuring all destination pixels are accounted for and handles overlaps naturally.</p>
</li>
</ul>
<p>Both methods involve interpolation since transformed coordinates often map to non-integer positions.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="blending"><a class="header" href="#blending">Blending</a></h1>
<p>Blending is essential in panorama stitching to smooth the transitions between overlapping images and avoid visible seams, color inconsistencies, or exposure differences. This is especially important when images were taken under varying conditions.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="common-blending-techniques"><a class="header" href="#common-blending-techniques">Common Blending Techniques</a></h1>
<ul>
<li>
<p><strong>Feathering</strong>:</p>
<ul>
<li><strong>Weighted Mask</strong>: A gradient mask is applied across the overlap, gradually blending the images. Pixels near the edge of the overlap have lower weight, while those toward the center have higher weight.</li>
<li><strong>Linear Interpolation</strong>: The pixel values in the overlap are computed by linearly interpolating between corresponding pixel values, weighted by the mask. This reduces visible seams but may cause ghosting if the images aren’t perfectly aligned.</li>
</ul>
</li>
<li>
<p><strong>Pyramid Blending</strong>:</p>
<ul>
<li><strong>Image Pyramids</strong>: Each image is decomposed into a multi-scale pyramid (e.g., using Gaussian pyramids) with progressively lower resolutions.</li>
<li><strong>Blend Each Layer</strong>: Corresponding layers from each pyramid are blended separately.</li>
<li><strong>Reconstruct</strong>: The final image is reconstructed from the blended pyramid layers. This method handles variations across different scales, reducing ghosting and improving smooth transitions.</li>
</ul>
</li>
<li>
<p><strong>Laplacian Pyramid Blending</strong>:</p>
<ul>
<li><strong>Laplacian Pyramids</strong>: These pyramids capture image details by subtracting each Gaussian pyramid level from the next, isolating finer details at each scale.</li>
<li><strong>Blend and Reconstruct</strong>: The Laplacian pyramids are blended, and the final image is reconstructed from the combined pyramid. This technique excels at preserving edge details, making it ideal for images with complex overlaps.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="3d-scene-reconstruction"><a class="header" href="#3d-scene-reconstruction">3D Scene Reconstruction</a></h1>
<p>3D scene reconstruction is the process of capturing the exact shape and appearance of real-world environments and objects in three dimensions using sequences of images or video.
This task involves converting observations from multiple two-dimensional images into a single three-dimensional model of a scene.</p>
<p>A typical 3D reconstruction pipeline involves:</p>
<ul>
<li>
<p><strong>Optical Flow</strong>: Provides a basis for understanding the apparent motion of objects in the image sequence. It can be a preliminary step in estimating how objects move relative to the camera. Optical flow helps differentiate between static backgrounds and moving objects in the scene. By understanding these differences, it's possible to segment the scene more accurately and reconstruct moving and static parts separately.</p>
</li>
<li>
<p><strong>Camera Calibration</strong>: Essential for determining the intrinsic parameters of the camera (such as focal length, optical center, and lens distortion). This step ensures that the 3D reconstruction is scaled and oriented correctly relative to the real world.</p>
</li>
<li>
<p><strong>Epipolar Geometry</strong>: Involves understanding the geometric relationship between multiple views of a scene taken from different camera positions. This is crucial for reducing errors in feature matching between images and simplifying algorithms by reducing the search space to 1D epipolar lines instead of 2D images.</p>
</li>
<li>
<p><strong>Stereo Matching</strong>: Uses epipolar geometry to find corresponding points between pairs of images taken from slightly different viewpoints. By identifying these correspondences, it’s possible to compute depth information via triangulation, leading to a dense 3D reconstruction of the scene.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="optical-flow"><a class="header" href="#optical-flow">Optical Flow</a></h1>
<p>Optical flow is a concept in computer vision that represents the pattern of apparent motion of objects, surfaces, and edges in a visual scene, as perceived through variations in image brightness across frames.
This apparent motion arises when there is relative movement between an observer (such as a camera) and the scene being observed.</p>
<p>In essence, optical flow captures the displacement of pixels between consecutive video frames, representing this motion as a vector field, where each vector indicates the direction and speed of motion at that point in the image.</p>
<p>Optical flow allows for the analysis of the dynamics within a scene
It is crucial for understanding how objects or features move over time, which is important for tasks such as object tracking and predicting future positions.</p>
<p>By analyzing the motion of points in a scene, optical flow can also contribute to the 3D reconstruction of scene geometry.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="optical-flow-vs-motion-field"><a class="header" href="#optical-flow-vs-motion-field">Optical Flow vs Motion Field</a></h1>
<p>In the context of optical flow and computer vision, the <strong>motion field</strong> refers to a 2D vector field that represents the projection of the actual 3D motion of points within a scene onto the image plane. This field indicates the real motion paths that points in the observed scene follow from one frame to another, due to either the movement of the camera, the objects, or both.</p>
<p>The motion field can arise from various sources, including the relative motion between the camera and the scene objects (e.g., a camera passing by stationary objects or rotating around a fixed point), or the independent motion of objects within the scene (e.g., cars moving on a road).</p>
<p>It geometrically represents how each point in three-dimensional space moves between frames in terms of two-dimensional vectors mapped onto the camera's image plane. The motion field attempts to capture real-world movement, as opposed to <strong>optical flow</strong>, which only captures <em>apparent</em> motion—how the motion appears to an observer, which can be influenced by factors like lighting changes, reflections, and other visual artifacts.</p>
<p>While the true motion field represents the actual physical movement of objects in 3D space, this information is typically inaccessible from a single viewpoint without additional data like depth cues or multiple camera views. Instead, what we can compute directly from image sequences is the optical flow, which represents the apparent motion in the 2D image plane.</p>
<p>Optical flow and the motion field ideally represent the same phenomenon—the movement of objects and features in a scene. However, optical flow, which is derived from changes in image brightness, does not always accurately reflect the actual physical motion described by the motion field.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="examples-of-discrepancies-between-optical-flow-and-motion-field"><a class="header" href="#examples-of-discrepancies-between-optical-flow-and-motion-field">Examples of Discrepancies Between Optical Flow and Motion Field</a></h1>
<ol>
<li>
<p><strong>Lambertian Motion Sphere</strong>:<br />
Imagine a perfectly smooth, Lambertian sphere (which reflects light diffusely) rotating in space. The sphere's surface points are moving, hence there is a real motion field. However, if the sphere is uniformly colored and the lighting is even, there might be no change in brightness patterns detectable by an observer. Therefore, no optical flow would be observed, even though there is an actual motion field.</p>
</li>
<li>
<p><strong>Moving Light Around a Stationary Ball</strong>:<br />
If the ball itself is stationary, there is no actual motion of the ball's surface points, and thus, the motion field is null. However, as the light moves, it creates changing shadows and highlights on the ball's surface. These changes in brightness are captured as optical flow, indicating apparent motion where there is no actual physical movement of the object.</p>
</li>
<li>
<p><strong>Barber Pole Illusion</strong>:<br />
The actual motion of the stripes on a barber pole is horizontal as the pole rotates around its axis. Visually, due to the cylindrical shape and the observer's usual frontal perspective, the stripes appear to move vertically. This creates an optical flow that is perpendicular to the actual direction of the motion field.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="optical-flow-constraint-equation"><a class="header" href="#optical-flow-constraint-equation">Optical Flow Constraint Equation</a></h1>
<p>When estimating optical flow, you generally work with two consecutive frames from a video sequence or two images taken at slightly different times, $t$ and $t + \Delta t$. The goal is to compute the motion between these frames—specifically, how each pixel or feature in the first frame moves to become a corresponding pixel or feature in the second frame.</p>
<p>These frames should be close enough in time to ensure minimal change in the scene other than the motion of interest.
This temporal closeness ensures that any movement between them is small and manageable. Another assumption is that the scene has stable lighting and no drastic environmental changes aside from object or camera movement.</p>
<p>In this setting, we make the following key assumptions:</p>
<ul>
<li>
<p><strong>Brightness Constancy Assumption</strong>:<br />
It is assumed that the brightness of any given point in the scene remains constant between the two frames. This means that if a point moves from one location to another between frames, its intensity does not change.
$$
I(x, y, t) = I(x + \Delta x, y + \Delta y, t + \Delta t)
$$</p>
</li>
<li>
<p><strong>Small Motion Assumption</strong>:<br />
Points in the image do not move far between frames, allowing for simpler mathematical treatment and avoiding large, complex displacements. This assumption permits the use of a first-order Taylor series to approximate changes, simplifying the problem to linear terms.</p>
</li>
<li>
<p><strong>Spatial Coherence Assumption</strong>:<br />
The motion of a pixel is assumed to be similar to that of its immediate neighbors. This assumption helps define smooth motion across the image and is critical in resolving ambiguities in areas where brightness constancy alone may be insufficient.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="derivation-of-the-optical-flow-constraint-equation"><a class="header" href="#derivation-of-the-optical-flow-constraint-equation">Derivation of the Optical Flow Constraint Equation</a></h1>
<p>Using the brightness constancy assumption and considering small motion, the image intensity function at the new location $(x + \Delta x, y + \Delta y)$ at time $(t + \Delta t)$ can be approximated using a Taylor series expansion:</p>
<p>$$
I(x + \Delta x, y + \Delta y, t + \Delta t) \approx I(x, y, t) + I_x \Delta x + I_y \Delta y + I_t \Delta t
$$</p>
<p>Where:</p>
<ul>
<li>$I_x = \frac{\partial I}{\partial x}$ is the image gradient in the $x$ direction,</li>
<li>$I_y = \frac{\partial I}{\partial y}$ is the image gradient in the $y$ direction,</li>
<li>$I_t = \frac{\partial I}{\partial t}$ is the temporal image gradient.</li>
</ul>
<p>By setting this approximation equal to the intensity at the original point under the brightness constancy assumption and rearranging, we obtain the <strong>optical flow constraint equation</strong>:</p>
<p>$$
I_x u + I_y v + I_t = 0
$$</p>
<p>Where $u = \frac{\Delta x}{\Delta t}$ and $v = \frac{\Delta y}{\Delta t}$ are the components of the optical flow velocity vector at a point.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="interpretation-of-the-optical-flow-constraint-equation"><a class="header" href="#interpretation-of-the-optical-flow-constraint-equation">Interpretation of the Optical Flow Constraint Equation</a></h1>
<p>This equation geometrically represents a line in the $u$-$v$ plane. Every point (velocity vector) that lies on this line is a potential solution to the optical flow constraint at a given pixel. Since we have only one equation in two unknowns ($u$ and $v$), there are infinitely many solutions that satisfy the equation for each pixel.</p>
<p>The true motion vector could be any point on this line. To determine the exact optical flow vector, additional information or constraints, such as smoothness assumptions or multiple viewpoints, are required.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="aperture-problem"><a class="header" href="#aperture-problem">Aperture Problem</a></h1>
<p>The aperture problem arises when motion information is available only within a limited field of view, which is common in scenarios where the camera (or aperture) captures only a small part of a larger scene.</p>
<p>This problem highlights a fundamental ambiguity in motion perception:</p>
<ul>
<li>
<p><strong>Limited Visibility</strong>:<br />
When viewing motion through a small aperture (literally or figuratively, such as a small window on a larger scene), it becomes challenging to discern the true direction of motion if the visible structure does not contain sufficient variation.</p>
</li>
<li>
<p><strong>Edge Motion</strong>:<br />
For instance, if you can only see a straight edge moving, without additional context or texture, you can only detect motion along the direction parallel to the edge. Motion perpendicular to the edge becomes indiscernible because the edge appears the same regardless of its movement along its length.</p>
</li>
</ul>
<p>Each instance of the optical flow equation provides only one constraint for the two unknown components of the motion vector (horizontal and vertical). This leads to multiple possible solutions for the true motion vector.</p>
<p>The optical flow calculation at any point depends on the local gradient of image brightness. In areas where this gradient is unidirectional (such as along an edge), the flow component perpendicular to this gradient remains undetermined, manifesting the aperture problem.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lucas-kanade-method"><a class="header" href="#lucas-kanade-method">Lucas-Kanade Method</a></h1>
<p>The Lucas-Kanade method assumes that the optical flow is essentially constant in a local neighborhood around each pixel. Instead of solving for the flow at each pixel independently, it finds a single flow vector that best fits all pixels within a window centered around the target pixel.</p>
<p>For each pixel in the window, the optical flow constraint equation is:</p>
<p>$$
I_x(x_i, y_i) u + I_y(x_i, y_i) v = -I_t(x_i, y_i)
$$</p>
<p>Where:</p>
<ul>
<li>$I_x(x_i, y_i)$ and $I_y(x_i, y_i)$ are the spatial image gradients,</li>
<li>$I_t(x_i, y_i)$ is the temporal image gradient,</li>
<li>$u$ and $v$ are the optical flow components.</li>
</ul>
<p>Since $u$ and $v$ are assumed to be constant across the window, this leads to a system of equations for all pixels in the window, which can be written in matrix form as:</p>
<p>$$
\begin{bmatrix}
I_x(x_1, y_1) &amp; I_y(x_1, y_1) \
I_x(x_2, y_2) &amp; I_y(x_2, y_2) \
\vdots &amp; \vdots \
I_x(x_n, y_n) &amp; I_y(x_n, y_n)
\end{bmatrix}
\begin{bmatrix}
u \
v
\end{bmatrix} =
\begin{bmatrix}
-I_t(x_1, y_1) \
-I_t(x_2, y_2) \
\vdots \
-I_t(x_n, y_n)
\end{bmatrix}
$$</p>
<p>Denoting the matrix of spatial gradients as $A$, the vector of temporal gradients as $\mathbf{b}$, and the flow vector as $\mathbf{u} = [u, v]^T$, the system can be written compactly as:</p>
<p>$$
A \mathbf{u} = \mathbf{b}
$$</p>
<p>The system is typically overdetermined, meaning there are more equations than unknowns, which makes it impossible to satisfy all equations exactly. Instead, the Lucas-Kanade method uses the least squares approach to find an optimal solution that minimizes the error across all equations:</p>
<p>$$
\mathbf{u} = (A^T A)^{-1} A^T \mathbf{b}
$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="conditions-for-a-unique-solution"><a class="header" href="#conditions-for-a-unique-solution">Conditions for a Unique Solution</a></h1>
<p>A unique solution exists if $A^T A$ is invertible. The matrix $A^T A$ is known as the <strong>second moment matrix</strong> (or autocorrelation matrix in some contexts), and it measures the spread or variance of the image gradients within the window. This matrix is symmetric and positive semi-definite.</p>
<p>The eigenvalues of $A^T A$ provide crucial information about the conditioning of the system. In the context of the Lucas-Kanade method:</p>
<ul>
<li>When both eigenvalues are large and approximately equal, the gradients vary significantly in multiple directions. This suggests a well-conditioned system, allowing for reliable estimation of both $u$ and $v$.</li>
<li>If one eigenvalue is much smaller than the other, it indicates that the gradients are primarily in one direction. This leads to the <strong>aperture problem</strong>, where motion orthogonal to the dominant gradient cannot be reliably determined.</li>
</ul>
<p>In cases where both eigenvalues are small, it implies insufficient variation in gradients, making the flow estimation unreliable.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="coarse-to-fine-flow-estimation"><a class="header" href="#coarse-to-fine-flow-estimation">Coarse-to-Fine Flow Estimation</a></h1>
<p>The assumption of small motion is crucial for the effectiveness of traditional optical flow methods like Lucas-Kanade. However, in scenarios with fast-moving objects or large displacements between frames, this assumption can fail, making it difficult for these methods to accurately compute motion. In such cases, a <strong>multi-scale, coarse-to-fine approach</strong> becomes highly valuable.</p>
<p>The coarse-to-fine strategy, also known as the <strong>pyramidal approach</strong>, involves creating a pyramid of images where each level is a down-sampled version of the original images, progressively reducing the resolution. By starting at the coarsest level (smallest image), the method first estimates the optical flow at this low resolution, where the apparent motion between frames is significantly reduced due to the smaller scale.</p>
<p>The steps in this process are as follows:</p>
<ol>
<li>
<p><strong>Build Image Pyramids</strong>:<br />
Both the current and next frames are processed to generate several layers of reduced-resolution images. Each layer is a down-sampled version of the previous one, forming an image pyramid.</p>
</li>
<li>
<p><strong>Initial Flow Estimation</strong>:<br />
Begin at the top of the pyramid (the coarsest, smallest images) and estimate the optical flow. At this level, even large motions become manageable because of the reduced image size.</p>
</li>
<li>
<p><strong>Refine Flow at Each Level</strong>:<br />
Use the flow estimate from the coarser level to guide the flow estimation at the next finer level down the pyramid. Typically, this refinement involves up-sampling the flow estimate from the coarser level and using it as an initial guess for the finer level.</p>
</li>
<li>
<p><strong>Iterate Down to the Finest Level</strong>:<br />
Continue refining the flow estimates down the pyramid until reaching the bottom, which corresponds to the original image resolution. At each level, the flow is progressively refined for increased accuracy.</p>
</li>
</ol>
<p>At each level, the estimated flow vectors are scaled appropriately to account for the down-sampling effect. This scaling ensures that when an estimated flow is up-sampled to the next finer level, it represents equivalent motion in the higher resolution space. Optical flow equations are solved at each level using methods like Lucas-Kanade, modified for the scale, or other algorithms suited to the specific resolution.</p>
<p>The coarse-to-fine approach significantly improves the accuracy of optical flow estimation in cases with large motions, as it allows for incremental refinement from a manageable approximation of the motion at lower resolutions.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="horn-schunck-method"><a class="header" href="#horn-schunck-method">Horn-Schunck Method</a></h1>
<p>The Horn-Schunck method estimates optical flow by modeling the image as a function of continuous variables $(x, y, t)$ and the flow fields $u$ and $v$ as continuous functions of $(x, y)$. The objective is to minimize the following energy functional:</p>
<p>$$
E(u,v) = \int \int \underbrace{(I(x+u(x,y), y+v(x,y), t+1) - I(x,y,t))^2}<em>\text{quadratic penalty for brightness change} +
\lambda \underbrace{(|| \nabla u(x,y) ||^2 + || \nabla v(x,y) ||^2) dxdy}</em>\text{quadratic penalty for flow smoothness}
$$</p>
<ul>
<li>The <strong>brightness constancy term</strong> ensures that the intensity of a point in the image remains constant over time, enforcing the assumption that pixels maintain their appearance as they move.</li>
<li>The <strong>smoothness term</strong> enforces smoothness in the flow field by penalizing large gradients in the flow vectors $u$ and $v$. This regularizes the flow, discouraging abrupt changes between neighboring pixels.</li>
</ul>
<p>The parameter $\lambda$ controls the trade-off between the brightness constancy and smoothness terms:</p>
<ul>
<li>A larger $\lambda$ enforces more smoothness but can oversmooth motion at object boundaries.</li>
<li>A smaller $\lambda$ allows more detailed motion but can produce noisy flow fields.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="balancing-data-fidelity-and-regularization"><a class="header" href="#balancing-data-fidelity-and-regularization">Balancing Data Fidelity and Regularization</a></h1>
<p>The Horn-Schunck method aims to balance two objectives:</p>
<ul>
<li><strong>Data Fidelity</strong>: Ensure that the computed flow respects the observed image intensities (brightness constancy).</li>
<li><strong>Regularization</strong>: Maintain smoothness in the flow field to avoid unrealistic discontinuities (flow smoothness).</li>
</ul>
<p>However, minimizing the energy functional is challenging due to its non-convex nature, which means finding the global minimum can be difficult.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="linearization-and-approximation"><a class="header" href="#linearization-and-approximation">Linearization and Approximation</a></h1>
<p>To simplify the minimization process, the brightness constancy term can be linearized using a first-order Taylor series expansion:</p>
<p>$$
I(x+u(x,y), y+v(x,y), t+1) \approx I(x, y, t) + I_x u(x,y) + I_y v(x,y) + I_t
$$</p>
<p>Where $I_x$, $I_y$, and $I_t$ are the spatial and temporal gradients of the image intensity. Substituting this into the energy functional gives:</p>
<p>$$
E(u,v) = \int \int \left( I_x(x,y) u(x,y) + I_y(x,y) v(x,y) + I_t(x,y) \right)^2 + \lambda \left( || \nabla u(x,y) ||^2 + || \nabla v(x,y) ||^2 \right) dxdy
$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="discretization-for-practical-use"><a class="header" href="#discretization-for-practical-use">Discretization for Practical Use</a></h1>
<p>To apply the Horn-Schunck method in practice, the continuous equation is discretized, turning the integrals into sums over the pixel grid of the image. The discretized objective function is:</p>
<p>$$
E(U, V) = \sum_{x, y} \left( (I_x(x, y) U(x, y) + I_y(x, y) V(x, y) + I_t(x, y))^2 \right) +
\lambda \sum_{x, y} \left( (U(x, y) - U(x+1, y))^2 + (U(x, y) - U(x, y+1))^2 + (V(x, y) - V(x+1, y))^2 + (V(x, y) - V(x, y+1))^2 \right)
$$</p>
<p>The resulting system of equations is large but sparse, involving only local neighborhoods of pixels. An iterative approach is commonly used to solve this system.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="iterative-solution-algorithm"><a class="header" href="#iterative-solution-algorithm">Iterative Solution Algorithm</a></h1>
<p>The algorithm for solving the Horn-Schunck optical flow problem works as follows:</p>
<ol>
<li><strong>Precompute image gradients</strong>: Compute $I_x$, $I_y$, and the temporal gradient $I_t$ from the input images.</li>
<li><strong>Initialize flow field</strong>: Set initial guesses for the flow fields, typically $(u, v) = (0, 0)$.</li>
<li><strong>Iterative updates</strong>: For each pixel, update the flow fields $u$ and $v$ iteratively based on neighboring pixel values.</li>
<li><strong>Repeat until convergence</strong>: Continue updating the flow fields until convergence, typically when changes between iterations fall below a predefined threshold.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="robustness-and-extensions"><a class="header" href="#robustness-and-extensions">Robustness and Extensions</a></h1>
<p>The traditional Horn-Schunck method tends to produce overly smooth flow fields, particularly at object boundaries, due to the regularization parameter $\lambda$. A high $\lambda$ value can obscure important motion discontinuities.</p>
<p>To make the method more robust, especially at discontinuities, alternative penalty functions or robust estimation techniques can be employed. The Horn-Schunck method can be framed as <strong>Maximum A Posteriori (MAP) inference</strong> in a <strong>Markov Random Field (MRF)</strong>, where the flow fields $(U, V)$ follow a probabilistic model:</p>
<p>$$
p(U,V) = \frac{1}{Z} \exp(-E(U,V))
$$</p>
<p>Here, $E(U,V)$ represents the Gibbs energy of the flow configuration, and minimizing this energy corresponds to finding the most likely flow field.</p>
<p>While a Gaussian model for the Gibbs distribution (associated with quadratic penalties) struggles with outliers and sharp motion boundaries, using a <strong>Student-t distribution</strong> introduces heavier tails and improves robustness. This allows for better handling of discontinuities while maintaining smoothness in other areas.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gradient-descent-for-optimization"><a class="header" href="#gradient-descent-for-optimization">Gradient Descent for Optimization</a></h1>
<p>Gradient descent is typically used to optimize the continuous flow fields $U$ and $V$. This approach allows the algorithm to adjust the flow vectors while maintaining sensitivity to the motion dynamics captured in the image sequence.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="optical-flow-estimation-with-deep-learning"><a class="header" href="#optical-flow-estimation-with-deep-learning">Optical Flow Estimation with Deep Learning</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="flownet-and-flownet2"><a class="header" href="#flownet-and-flownet2">FlowNet and FlowNet2</a></h1>
<p>FlowNet is a pioneering CNN architecture for end-to-end optical flow estimation, where the entire process—from input frames to flow fields—is handled by a single model. It uses an encoder-decoder structure, with the encoder capturing abstract representations and the decoder reconstructing the flow field.</p>
<p>Two variants of FlowNet exist:</p>
<ul>
<li><strong>FlowNetSimple</strong>: Stacks two frames and processes them through a single network to directly predict the optical flow.</li>
<li><strong>FlowNetCorr</strong>: Uses two streams for the input frames, combining them with a correlation layer to learn displacement between the images.</li>
</ul>
<p><strong>FlowNet2</strong> improves on FlowNet by stacking multiple FlowNets, each refining the flow estimate. It introduces sub-networks to better handle large displacements, enhancing accuracy over the original model.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pwc-net"><a class="header" href="#pwc-net">PWC-Net</a></h1>
<p>PWC-Net (Pyramid, Warping, Cost volume) builds on several innovations for optical flow estimation. It processes images through a pyramid of feature representations, capturing motion at multiple scales. Large displacements are handled at coarser levels, while finer details are refined at higher resolutions.</p>
<p>The <strong>warping</strong> step aligns the second image with the first using the flow estimated from coarser levels, simplifying displacement estimation. A <strong>cost volume</strong> is then computed to measure the similarity between the warped image and the first image, helping estimate motion.</p>
<p>Starting from the coarsest level, PWC-Net iteratively refines the optical flow at each level of the pyramid, with the final output being the full-resolution flow field.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="feature-tracking"><a class="header" href="#feature-tracking">Feature Tracking</a></h1>
<p>Feature tracking in computer vision involves detecting and following distinctive points (features) across a series of images or video frames. The goal is to track these points as they move through space and time, maintaining their identity across frames.</p>
<p>Once features are detected, described, and matched between frames, the motion of each matched feature can be tracked. This involves estimating the motion vector or transformation that aligns each feature with its match in subsequent frames.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lucas-kanade-feature-tracker"><a class="header" href="#lucas-kanade-feature-tracker">Lucas-Kanade Feature Tracker</a></h1>
<p>The Lucas-Kanade method estimates the motion of selected features by assuming that the optical flow of the brightness pattern in the image window remains constant over short time intervals. It calculates the displacement vector (in the x and y directions) for each feature, minimizing the appearance difference in its neighborhood between consecutive frames.</p>
<p>By using a multi-scale (pyramidal) approach, the tracker can handle large motions, refining these estimates at finer scales. This results in a set of flow vectors for each tracked feature, indicating its movement from one frame to the next.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="camera-models"><a class="header" href="#camera-models">Camera Models</a></h1>
<p>A camera model is a mathematical framework that describes how a camera projects a 3D scene onto a 2D image plane. It defines the relationship between 3D world coordinates and their corresponding 2D image coordinates, using both intrinsic and extrinsic parameters.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="intrinsic-parameters"><a class="header" href="#intrinsic-parameters">Intrinsic Parameters</a></h1>
<p>Intrinsic parameters define the internal characteristics of the camera, including:</p>
<ul>
<li><strong>Focal length (f)</strong>: The distance between the camera sensor and the lens, affecting the field of view and magnification.</li>
<li><strong>Principal point (c_x, c_y)</strong>: The point on the image sensor corresponding to the camera's optical center.</li>
<li><strong>Skew coefficient</strong>: Describes the skewness between the x and y pixel axes, ideally zero in well-calibrated cameras.</li>
<li><strong>Distortion coefficients</strong>: Parameters that account for lens distortions, including radial and tangential distortions that warp the image.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="extrinsic-parameters"><a class="header" href="#extrinsic-parameters">Extrinsic Parameters</a></h1>
<p>Extrinsic parameters define the camera's position and orientation in the world:</p>
<ul>
<li><strong>Rotation matrix (R)</strong>: Describes the camera's orientation relative to the world.</li>
<li><strong>Translation vector (t)</strong>: Defines the camera's position in the world relative to a reference point.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pinhole-camera-model"><a class="header" href="#pinhole-camera-model">Pinhole Camera Model</a></h1>
<p>The pinhole camera model is a simplified and idealized representation that assumes an infinitely small aperture (pinhole) through which light rays pass to form an image on the image plane. It ignores lens effects like distortion, making it a practical model for many applications due to its simplicity.</p>
<p>In this model, 3D points from the scene are projected through the pinhole onto a 2D image plane, with depth relationships preserved—distant objects appear smaller than closer ones. The image is formed behind the pinhole at a distance determined by the <strong>focal length (f)</strong>.</p>
<p>Since the pinhole camera model does not account for lens distortion, it is ideal for scenarios where precise projection without optical corrections is needed.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="forward-imaging-model"><a class="header" href="#forward-imaging-model">Forward Imaging Model</a></h1>
<p>The forward imaging model describes how 3D points in the world are projected onto a 2D image sensor using a set of mathematical transformations. It accounts for both <strong>intrinsic</strong> parameters (e.g., focal length, principal point, and distortion) and <strong>extrinsic</strong> parameters (position and orientation of the camera) to accurately map 3D coordinates to 2D image coordinates.</p>
<p>The forward imaging model consists of two main steps:</p>
<ol>
<li><strong>Coordinate transformation</strong>: Transforms world coordinates (3D points in space) into camera coordinates.</li>
<li><strong>Perspective projection</strong>: Projects the camera coordinates onto the 2D image plane.</li>
</ol>
<p>Unlike the idealized pinhole camera model, the forward imaging model corrects for real-world effects like lens distortion and imperfections, providing a more accurate depiction of how cameras capture images.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="perspective-projection"><a class="header" href="#perspective-projection">Perspective Projection</a></h1>
<p>In the camera coordinate system, real-world measurements (e.g., in millimeters) must be converted to pixel units for image coordinates. This conversion uses scaling factors $m_x$ and $m_y$, which translate sensor measurements to pixels:</p>
<p>$$
m_x = \frac{\text{Number of pixels in x-axis}}{\text{Sensor width in mm}}, \quad
m_y = \frac{\text{Number of pixels in y-axis}}{\text{Sensor height in mm}}
$$</p>
<p>The <strong>principal point</strong> $(o_x, o_y)$ is where the camera's optical axis intersects the image plane, typically near the center. The <strong>focal length</strong>, converted to pixel units using $m_x$ and $m_y$, scales the 3D coordinates based on their depth ($Z$-coordinate):</p>
<p>$$
x' = m_x\frac{f \cdot X}{Z} + o_x, \quad
y' = m_y\frac{f \cdot Y}{Z} + o_y
$$</p>
<p>To express this projection using matrix operations, we use <strong>homogeneous coordinates</strong>. The <strong>intrinsic matrix</strong> $K$ encapsulates the camera's internal parameters:</p>
<p>$$
\begin{pmatrix}
x \
y \
w
\end{pmatrix} =
\begin{pmatrix}
f \cdot m_x &amp; 0 &amp; o_x &amp; 0 \
0 &amp; f \cdot m_y &amp; o_y &amp; 0 \
0 &amp; 0 &amp; 1 &amp; 0
\end{pmatrix}
\begin{pmatrix}
X \
Y \
Z \
1
\end{pmatrix}
$$</p>
<p>The intrinsic matrix $K$ is part of the full projection matrix and is expressed as:</p>
<p>$$
K = \begin{pmatrix}
f \cdot m_x &amp; 0 &amp; o_x \
0 &amp; f \cdot m_y &amp; o_y \
0 &amp; 0 &amp; 1
\end{pmatrix}
$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="coordinate-transformation"><a class="header" href="#coordinate-transformation">Coordinate Transformation</a></h1>
<p>Transforming world coordinates to camera coordinates involves the <strong>extrinsic parameters</strong>, which describe the camera's orientation and position in the world:</p>
<ul>
<li><strong>Rotation matrix (R)</strong>: A 3x3 matrix representing the camera's orientation.</li>
<li><strong>Translation vector (t)</strong>: A vector representing the camera's position.</li>
</ul>
<p>The transformation from world coordinates $P_W$ to camera coordinates $P_C$ is:</p>
<p>$$
P_C = R(P_W - t)
$$</p>
<p>In homogeneous coordinates, this becomes:</p>
<p>$$
\mathbf{E} = \begin{bmatrix}
R &amp; \mathbf{t} \
\mathbf{0}^T &amp; 1
\end{bmatrix}
$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="full-projection-matrix"><a class="header" href="#full-projection-matrix">Full Projection Matrix</a></h1>
<p>The full projection matrix $P$ combines both intrinsic and extrinsic transformations, mapping 3D world points to 2D image coordinates:</p>
<p>$$
\mathbf{P} = \mathbf{K} \mathbf{E}
$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lens-distortion"><a class="header" href="#lens-distortion">Lens Distortion</a></h1>
<p>Real cameras introduce <strong>lens distortion</strong>, causing non-linear displacements of points in the image. The two main types are:</p>
<ul>
<li><strong>Radial distortion</strong>: Affects points based on their distance from the optical center, leading to compression or stretching near the edges.</li>
<li><strong>Tangential distortion</strong>: Occurs when the lens is misaligned with the sensor, causing a tilting effect.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="camera-calibration"><a class="header" href="#camera-calibration">Camera Calibration</a></h1>
<p>Camera calibration is the process of determining the intrinsic and extrinsic parameters of a specific camera so that it aligns with a mathematical model. Calibration uses real-world images of known patterns (e.g., checkerboards) to compute the camera’s parameters, including lens distortions, ensuring accurate 3D-to-2D transformations.</p>
<p>The calibration process adjusts the mathematical model to fit the camera's physical characteristics, accounting for imperfections such as lens distortion and sensor misalignment.</p>
<p>For each 3D world point $P_W$ and 2D image point $p$, the projection equation is:</p>
<p>$$
u = \frac{p_{11} X + p_{12} Y + p_{13} Z + p_{14}}{p_{31} X + p_{32} Y + p_{33} Z + p_{34}}, \quad
v = \frac{p_{21} X + p_{22} Y + p_{23} Z + p_{24}}{p_{31} X + p_{32} Y + p_{33} Z + p_{34}}
$$</p>
<p>Multiplying both sides by the denominator to avoid division gives:</p>
<p>$$
u \left( p_{31} X + p_{32} Y + p_{33} Z + p_{34} \right) = p_{11} X + p_{12} Y + p_{13} Z + p_{14}
$$
$$
v \left( p_{31} X + p_{32} Y + p_{33} Z + p_{34} \right) = p_{21} X + p_{22} Y + p_{23} Z + p_{24}
$$</p>
<p>These equations can be represented as a system for each point correspondence:</p>
<p>$$
\mathbf{A} \mathbf{p} = 0
$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="solving-the-overdetermined-system"><a class="header" href="#solving-the-overdetermined-system">Solving the Overdetermined System</a></h1>
<p>In practice, the system $\mathbf{A}\mathbf{p} = 0$ is overdetermined because we have many point correspondences but only 12 unknowns in $\mathbf{p}$. Overdetermined systems have no exact solution due to:</p>
<ul>
<li><strong>Measurement noise</strong>: Real-world data introduces errors.</li>
<li><strong>Imperfections</strong>: Inaccuracies in image measurements and object geometry.</li>
</ul>
<p>To find the best projection matrix $\mathbf{P}$, we solve the system using the <strong>least squares method</strong>, which minimizes the squared errors between the observed and projected points:</p>
<p>$$
\min || \mathbf{A} \mathbf{p} ||^2
$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="svd-and-qr-factorization"><a class="header" href="#svd-and-qr-factorization">SVD and QR Factorization</a></h1>
<p>The least squares solution is obtained using <strong>Singular Value Decomposition (SVD)</strong>, which decomposes $\mathbf{A}$ into its singular vectors and singular values. Once the solution is found, <strong>QR factorization</strong> is used to extract the intrinsic and extrinsic parameters from the projection matrix.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="epipolar-geometry"><a class="header" href="#epipolar-geometry">Epipolar Geometry</a></h1>
<p>Epipolar geometry describes the geometric relationship between two views of a 3D scene, observed by two cameras. It's crucial in stereo vision, as it simplifies the task of finding corresponding points between images for 3D reconstruction, motion estimation, and object recognition.</p>
<p>The <strong>fundamental matrix (F)</strong> is a 3x3 matrix that encapsulates epipolar geometry between two images. Given a point in one image, the corresponding epipolar line in the other image can be computed using the fundamental matrix. It is central to uncalibrated stereo vision, where the camera intrinsics are unknown.</p>
<p>For <strong>calibrated systems</strong>, where the camera intrinsics are known, the <strong>essential matrix (E)</strong> encodes the relative rotation and translation (extrinsics) between two camera views. It provides a more constrained basis for estimating the relative pose between cameras.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="key-elements-of-epipolar-geometry"><a class="header" href="#key-elements-of-epipolar-geometry">Key Elements of Epipolar Geometry</a></h1>
<ul>
<li><strong>Epipolar Plane</strong>: The plane that contains the baseline (line connecting the camera centers) and a point in the scene observed by both cameras.</li>
<li><strong>Epipole</strong>: The intersection of the baseline with the image plane of each camera. The epipole in one camera is the projection of the other camera's center.</li>
<li><strong>Epipolar Line</strong>: The line where the epipolar plane intersects the image plane. It constrains the search for corresponding points to this line, reducing the problem from 2D to 1D.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="essential-matrix"><a class="header" href="#essential-matrix">Essential Matrix</a></h1>
<p>In stereo vision, the <strong>epipolar constraint</strong> defines the relationship between corresponding points in two views using the essential matrix. Given two corresponding points $\mathbf{x}$ in the first image and $\mathbf{x}'$ in the second image, the epipolar constraint is:</p>
<p>$$
\mathbf{x}'^T E \mathbf{x} = 0
$$</p>
<p>For a 3D point $\mathbf{X}$ observed in two cameras, the transformation between the two camera views can be expressed as:</p>
<p>$$
\mathbf{x}' = R (\mathbf{x} - \mathbf{t})
$$</p>
<p>Where $R$ is the rotation matrix and $\mathbf{t}$ is the translation vector between the two camera views.</p>
<p>Using the coplanarity condition, the essential matrix $E$ is derived as:</p>
<p>$$
E = R [\mathbf{t}]_{\times}
$$</p>
<p>Where $[\mathbf{t}]_{\times}$ is the skew-symmetric matrix of the translation vector $\mathbf{t}$:</p>
<p>$$
[\mathbf{t}]_{\times} = \begin{bmatrix} 0 &amp; -t_3 &amp; t_2 \ t_3 &amp; 0 &amp; -t_1 \ -t_2 &amp; t_1 &amp; 0 \end{bmatrix}
$$</p>
<p>Thus, the epipolar constraint becomes:</p>
<p>$$
\mathbf{x}'^T E \mathbf{x} = 0
$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="properties-of-the-essential-matrix"><a class="header" href="#properties-of-the-essential-matrix">Properties of the Essential Matrix</a></h1>
<ol>
<li><strong>Longuet-Higgins equation</strong>: The epipolar constraint $\mathbf{x}'^T E \mathbf{x} = 0$ must be satisfied by corresponding points in two images.</li>
<li><strong>Epipolar lines</strong>: For a point $\mathbf{x}$ in one image, the corresponding epipolar line in the other image is given by:
$$
\mathbf{l}' = E \mathbf{x}, \quad \mathbf{l} = E^T \mathbf{x}'
$$</li>
<li><strong>Epipoles</strong>: The epipoles are the null spaces of $E$ and $E^T$, respectively:
$$
E \mathbf{e} = 0, \quad \mathbf{e}'^T E = 0
$$</li>
</ol>
<p>The essential matrix has <strong>five degrees of freedom</strong> (three for rotation and two for translation), and it is <strong>rank 2</strong>, meaning its two non-zero singular values are equal.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fundamental-matrix"><a class="header" href="#fundamental-matrix">Fundamental Matrix</a></h1>
<p>The <strong>fundamental matrix (F)</strong> is a generalization of the essential matrix for <strong>uncalibrated stereo systems</strong>, where the camera intrinsics are unknown. It relates corresponding points $\mathbf{x}$ and $\mathbf{x}'$ between two images:</p>
<p>$$
\mathbf{x}'^T F \mathbf{x} = 0
$$</p>
<p>When the intrinsic camera parameters $K$ and $K'$ are known, the fundamental matrix can be derived from the essential matrix as:</p>
<p>$$
F = K'^{-T} E K^{-1}
$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="properties-of-the-fundamental-matrix"><a class="header" href="#properties-of-the-fundamental-matrix">Properties of the Fundamental Matrix</a></h1>
<ol>
<li><strong>Epipolar constraint</strong>: $\mathbf{x}'^T F \mathbf{x} = 0$ must hold for corresponding points in two images.</li>
<li><strong>Epipolar lines</strong>: Similar to the essential matrix, the epipolar lines for corresponding points are:
$$
\mathbf{l}' = F \mathbf{x}, \quad \mathbf{l} = F^T \mathbf{x}'
$$</li>
<li><strong>Epipoles</strong>: The epipoles satisfy:
$$
F \mathbf{e} = 0, \quad \mathbf{e}'^T F = 0
$$</li>
</ol>
<p>The fundamental matrix has <strong>eight degrees of freedom</strong> (9 parameters, minus 1 for scale). Like the essential matrix, it is <strong>rank 2</strong>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="8-point-algorithm-for-fundamental-matrix"><a class="header" href="#8-point-algorithm-for-fundamental-matrix">8-Point Algorithm for Fundamental Matrix</a></h1>
<p>The <strong>eight-point algorithm</strong> is a method to compute the fundamental matrix using at least 8 point correspondences between two images.</p>
<p>Steps:</p>
<ol>
<li><strong>Normalize the points</strong>: Translate and scale the points so that the centroid is at the origin and the average distance to the origin is $\sqrt{2}$.</li>
<li><strong>Set up the system of equations</strong>: Each point correspondence provides one linear equation:
$$
x'_m x_m f_1 + x'_m y_m f_2 + x'_m f_3 + y'_m x_m f_4 + y'_m y_m f_5 + y'_m f_6 + x_m f_7 + y_m f_8 + f_9 = 0
$$</li>
<li><strong>Assemble the matrix $A$</strong>: Using the point correspondences, form the matrix $A$.</li>
<li><strong>Solve using SVD</strong>: Compute the SVD of $A$ and take the smallest singular value.</li>
<li><strong>Enforce the rank-2 constraint</strong>: Modify $F$ by setting the smallest singular value to 0.</li>
<li><strong>Unnormalize</strong>: Transform $F$ back to the original scale.</li>
</ol>
<p>The result is the fundamental matrix $F$ that best satisfies the epipolar constraint for the given point correspondences.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="stereo-matching"><a class="header" href="#stereo-matching">Stereo Matching</a></h1>
<p>Stereo matching involves finding corresponding points between two stereo images taken from slightly different viewpoints. The goal is to compute the <strong>disparity</strong> for each pair of corresponding points, which is the difference in their horizontal positions in the two images.</p>
<p>The cameras are typically aligned so that their imaging planes are parallel, and corresponding points lie on the same horizontal lines (epipolar lines). The <strong>baseline</strong> $b$ is the known distance between the two camera centers.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="disparity-and-depth"><a class="header" href="#disparity-and-depth">Disparity and Depth</a></h1>
<p>Disparity ($d$) is defined as the horizontal shift between corresponding points in the left ($u_l$) and right ($u_r$) images:</p>
<p>$$
d = u_l - u_r
$$</p>
<p>The <strong>depth</strong> $Z$ of a point is related to disparity, the baseline $b$, and the camera focal length $f$ by:</p>
<p>$$
Z = \frac{f \cdot b}{d}
$$</p>
<ul>
<li><strong>Greater disparity</strong> indicates a closer object.</li>
<li><strong>Lesser disparity</strong> indicates a farther object.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-rectification"><a class="header" href="#image-rectification">Image Rectification</a></h1>
<p>If the images are not aligned, <strong>rectification</strong> transforms the images to align the epipolar lines horizontally, simplifying stereo matching. Rectification involves applying homographies to warp the images so that corresponding points lie on the same horizontal lines. This is achieved by decomposing the essential matrix $E$ into rotation ($R$) and translation ($t$), then computing rectifying homographies.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="local-stereo-matching"><a class="header" href="#local-stereo-matching">Local Stereo Matching</a></h1>
<p>Local stereo matching computes disparity by comparing local image patches. The steps are:</p>
<ol>
<li><strong>Disparity Range</strong>: Set a range of possible disparities.</li>
<li><strong>Block Matching</strong>: Compare blocks of pixels in the left and right images over the disparity range.</li>
<li><strong>Cost Calculation</strong>: Compute similarity using metrics like <strong>Sum of Absolute Differences (SAD)</strong>.</li>
<li><strong>Disparity Selection</strong>: Choose the disparity with the lowest cost for each pixel.</li>
</ol>
<p>This method is fast but may struggle in low-texture areas or repetitive patterns.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="template-matching"><a class="header" href="#template-matching">Template Matching</a></h1>
<p>Template matching for stereo vision finds the correspondence between a small window or patch of pixels in one image and a patch in the other image of a stereo pair. The main steps are:</p>
<ul>
<li>A template is defined in one image (usually the left image), and the algorithm searches for the most similar template along the corresponding epipolar line in the other image.</li>
<li>The similarity between the template and candidate patches in the right image is measured using metrics like <strong>Normalized Cross-Correlation (NCC)</strong> or <strong>Sum of Squared Differences (SSD)</strong>.</li>
<li>The best match corresponds to the position with the highest (for NCC) or lowest (for SSD) similarity score.</li>
<li>The disparity for each pixel is the horizontal difference between the matched patches, which can be related to depth based on camera geometry and baseline.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="disparity-space-image-dsi"><a class="header" href="#disparity-space-image-dsi">Disparity Space Image (DSI)</a></h1>
<p>The <strong>Disparity Space Image (DSI)</strong> represents match scores between pixel patches from two stereo images across different disparities. For each pixel in the left image, it stores the matching cost for each possible disparity.</p>
<ul>
<li>The DSI is used to find the disparity that minimizes the cost for each pixel.</li>
<li>DSI can be visualized as a matrix where lower values indicate better matches.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="stereo-block-matching"><a class="header" href="#stereo-block-matching">Stereo Block Matching</a></h1>
<p>In block matching, small blocks around each pixel are compared across a range of disparities to find the best match. The <strong>disparity</strong> that minimizes the cost is selected as the correct match.</p>
<ol>
<li>Set a disparity range $[0, D]$.</li>
<li>For each block in the left image, slide it along the corresponding row in the right image.</li>
<li>Calculate the similarity score for each position.</li>
<li>Select the disparity with the best score.</li>
<li>Apply <strong>left-right consistency</strong> checks to improve reliability.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="non-local-stereo-matching"><a class="header" href="#non-local-stereo-matching">Non-Local Stereo Matching</a></h1>
<p>Non-local algorithms incorporate more extensive spatial contexts or even global image information to determine disparity. These methods often use:</p>
<ul>
<li><strong>Dynamic Programming</strong>: Incorporates a smoothness constraint along a scanline to find an optimal match by minimizing a global cost function.</li>
<li><strong>Graph Cuts</strong>: Models the problem as a graph where each pixel is a node, and disparity choices are modeled as edges. The goal is to find a cut that minimizes the total disparity error across the image.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dsi-with-inter-scanline-consistency"><a class="header" href="#dsi-with-inter-scanline-consistency">DSI with Inter-Scanline Consistency</a></h1>
<p>Inter-scanline consistency enforces continuity in the disparity mapping across consecutive rows or scanlines. This is crucial because disparities tend to change gradually, except at object boundaries or occlusions.</p>
<ol>
<li><strong>Match Scoring</strong>: Compute similarity or dissimilarity scores (e.g., SAD or NCC) for each pixel in the left scanline compared to the right, recording the results in the DSI.</li>
<li><strong>Path Finding</strong>: Using dynamic programming algorithms like the Viterbi algorithm, find a smooth path through the DSI that minimizes the total dissimilarity score.</li>
<li><strong>Consistency Check</strong>: A left-right consistency check can verify that the disparity obtained from one image matches the other, helping to detect occlusions.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="stereo-matching-as-an-energy-minimization-problem"><a class="header" href="#stereo-matching-as-an-energy-minimization-problem">Stereo Matching as an Energy Minimization Problem</a></h1>
<p>Stereo matching can be formulated as an energy minimization problem. The goal is to find the disparity for each pixel that minimizes a global energy function, which typically includes terms for both image similarity and smoothness.</p>
<p>The energy function $E(d)$ is a combination of a data term and a smoothness term:</p>
<p>$$
E(d) = E_d(d) + \lambda E_s(d)
$$</p>
<p>Where:</p>
<ul>
<li>$E_d(d)$ is the <strong>data term</strong>, measuring the similarity between corresponding pixels in the left and right images.</li>
<li>$E_s(d)$ is the <strong>smoothness term</strong>, penalizing large disparity differences between neighboring pixels.</li>
<li>$\lambda$ is a weighting parameter.</li>
</ul>
<p>The <strong>data term</strong> can be computed as the Sum of Squared Differences (SSD) between pixel intensities in corresponding windows:</p>
<p>$$
E_d(d) = \sum_{(x, y) \in I} C(x, y, d(x, y))
$$</p>
<p>Where $C(x, y, d(x, y))$ is the cost of assigning disparity $d(x, y)$ to pixel $(x, y)$:</p>
<p>$$
C(x, y, d) = \text{SSD}(\text{window centered at } (x, y) \text{ in left image}, \text{window centered at } (x + d, y) \text{ in right image})
$$</p>
<p>The <strong>smoothness term</strong> is:</p>
<p>$$
E_s(d) = \sum_{(p, q) \in \mathcal{E}} V(d_p, d_q)
$$</p>
<p>Where $V(d_p, d_q)$ is a penalty function applied to the disparity values of neighboring pixels $p$ and $q$. A common choice is the L1 norm or the Potts model.</p>
<p>The dynamic programming approach finds disparities for all pixels such that the total energy $E(d)$ across the image is minimized. This is done by recursively calculating:</p>
<p>$$
D(x, y, d) = C(x, y, d) + \min_{d'} \left{ D(x - 1, y, d') + \lambda \left| d - d' \right| \right}
$$</p>
<p>Where $D(x, y, d)$ represents the cumulative cost of assigning disparity $d$ to pixel $(x, y)$. The recursion ensures that both the immediate cost and the transition cost (which enforces smoothness) are considered, resulting in a globally optimized disparity map.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deep-models-for-stereo-matching"><a class="header" href="#deep-models-for-stereo-matching">Deep Models for Stereo Matching</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="siamese-networks"><a class="header" href="#siamese-networks">Siamese Networks</a></h1>
<p>Siamese networks for stereo matching use convolutional neural networks (CNNs) to extract features from stereo image pairs. These features are then correlated, often via dot product, to measure similarity between corresponding points. Disparity is determined by selecting the maximum correlation score for each pixel, with options for further refinement using global optimization techniques. This deep learning approach improves robustness in handling occlusions and textureless regions compared to traditional methods.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dispnet"><a class="header" href="#dispnet">DispNet</a></h1>
<p>DispNet is the first end-to-end deep neural network designed for stereo matching. Its architecture, inspired by U-Net, includes a contracting path to capture context and an expanding path for precise localization, with skip connections for better detail retention. A correlation layer processes disparities to compute similarity between patches of stereo images. Using multi-scale loss, DispNet is trained to handle varying levels of difficulty, improving accuracy across diverse stereo vision tasks.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="stereo-mixture-density-networks-smd-nets"><a class="header" href="#stereo-mixture-density-networks-smd-nets">Stereo Mixture Density Networks (SMD-Nets)</a></h1>
<p>SMD-Nets use mixture density networks to predict high-resolution disparity maps by modeling the probability distribution of disparities for each pixel. This probabilistic approach improves handling of ambiguities and occlusions, preserving edge details and structural integrity in complex scenes. SMD-Nets provide sharper and more accurate disparity maps, particularly in challenging environments.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="triangulation"><a class="header" href="#triangulation">Triangulation</a></h1>
<p>Triangulation is a crucial concept in stereo vision used to determine the three-dimensional coordinates of a point from its projections in two (or more) images taken from different viewpoints. It involves identifying the 3D point that corresponds to a specific pair of 2D points observed from different camera positions.</p>
<p>Given a set of (noisy) matched points ${\mathbf{x}_i, \mathbf{x}_i'}$ and camera matrices ${\mathbf{P}, \mathbf{P}'}$, the goal is to estimate the 3D point $\mathbf{X}$.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="projection-equation"><a class="header" href="#projection-equation">Projection Equation</a></h1>
<p>With known projection matrices, the projection equation is:</p>
<p>$$
\mathbf{x} = \mathbf{P} \mathbf{X}
$$</p>
<p>In real-world scenarios, we observe the scaled 2D coordinates $\mathbf{x}$ (not homogeneous coordinates). The relationship between the 2D projection and the 3D point can be expressed with a scale factor $\alpha$:</p>
<p>$$
\mathbf{x} = \alpha \mathbf{P} \mathbf{X}
$$</p>
<p>Here, $\alpha$ is the unknown scale factor relating the 2D coordinates to the homogeneous projection. This reflects that $\mathbf{x}$ and $\mathbf{P} \mathbf{X}$ are collinear vectors.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="eliminating-the-scale-factor"><a class="header" href="#eliminating-the-scale-factor">Eliminating the Scale Factor</a></h1>
<p>To eliminate the scale factor, we use a geometrical trick: the cross product of two collinear vectors is zero:</p>
<p>$$
\mathbf{x} \times (\mathbf{P} \mathbf{X}) = 0
$$</p>
<p>Assuming $\mathbf{x} = [x \ y \ 1]^T$, the cross product of $\mathbf{x}$ and $\mathbf{P}\mathbf{X}$ expands as:</p>
<p>$$
\mathbf{x} \times \mathbf{P} \mathbf{X} = \begin{bmatrix}
y \mathbf{p}_3^T \mathbf{X} - \mathbf{p}_2^T \mathbf{X} \
\mathbf{p}_1^T \mathbf{X} - x \mathbf{p}_3^T \mathbf{X} \
x \mathbf{p}_2^T \mathbf{X} - y \mathbf{p}_1^T \mathbf{X}
\end{bmatrix} = \mathbf{0}
$$</p>
<p>Where $\mathbf{p}_1^T$, $\mathbf{p}_2^T$, and $\mathbf{p}_3^T$ are the rows of the projection matrix $\mathbf{P}$. This gives us three equations, but due to homogeneity, only two are independent.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="system-of-equations"><a class="header" href="#system-of-equations">System of Equations</a></h1>
<p>For each 2D-3D point correspondence, we get two independent equations. For a single image point $\mathbf{x}$ and its corresponding 3D point $\mathbf{X}$, the system of equations looks like this:</p>
<p>$$
\begin{bmatrix}
y \mathbf{p}_3^T - \mathbf{p}_2^T \
\mathbf{p}_1^T - x \mathbf{p}_3^T
\end{bmatrix} \mathbf{X} = \mathbf{0}
$$</p>
<p>If we have multiple cameras (or two views), we can concatenate the equations from each view to form a larger system. For example, with two cameras and two projection matrices, we get:</p>
<p>$$
\begin{bmatrix}
y_1 \mathbf{p}<em>{13}^T - \mathbf{p}</em>{12}^T \
\mathbf{p}<em>{11}^T - x_1 \mathbf{p}</em>{13}^T \
y_2 \mathbf{p}<em>{23}^T - \mathbf{p}</em>{22}^T \
\mathbf{p}<em>{21}^T - x_2 \mathbf{p}</em>{23}^T
\end{bmatrix} \mathbf{X} = \mathbf{0}
$$</p>
<p>This forms a homogeneous system of equations $\mathbf{A} \mathbf{X} = 0$.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="solving-the-system"><a class="header" href="#solving-the-system">Solving the System</a></h1>
<p>To find the 3D point $\mathbf{X}$, we solve the homogeneous system. Since it is a homogeneous system, the solution is typically found using <strong>Singular Value Decomposition (SVD)</strong>. The solution to such a system is given by the smallest singular value of $\mathbf{A}$.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="handling-noise"><a class="header" href="#handling-noise">Handling Noise</a></h1>
<p>In an ideal scenario without noise, the rays projected from different cameras through corresponding points should intersect exactly at the location of the 3D point. However, due to noise in the data (e.g., pixel quantization, camera calibration errors), these rays often do not intersect perfectly.</p>
<p>Since the rays do not intersect perfectly due to noise, the <strong>least squares method</strong> is used to find the best point that minimizes the error in terms of the distance from all rays. This method computes an optimal solution that is closest to satisfying all the equations given by the projection matrices.</p>
<p>Mathematically, this involves setting up a system of equations derived from each camera's projection equation and solving it by minimizing the sum of squared differences (errors) between the observed projections and the projections predicted by the model.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deep-learning"><a class="header" href="#deep-learning">Deep Learning</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-recognition"><a class="header" href="#image-recognition">Image Recognition</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-classification"><a class="header" href="#image-classification">Image Classification</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="first-datasets"><a class="header" href="#first-datasets">First Datasets</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="early-classifiers-and-approaches"><a class="header" href="#early-classifiers-and-approaches">Early Classifiers and Approaches</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deep-learning-classifiers-and-approaches"><a class="header" href="#deep-learning-classifiers-and-approaches">Deep Learning Classifiers and Approaches</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="top-networks-in-image-classification"><a class="header" href="#top-networks-in-image-classification">Top Networks in Image Classification</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="semantic-segmentation"><a class="header" href="#semantic-segmentation">Semantic Segmentation</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="approach"><a class="header" href="#approach">Approach</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="problems-with-standard-fcns"><a class="header" href="#problems-with-standard-fcns">Problems with Standard FCNs</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="solution-learnable-upsampling"><a class="header" href="#solution-learnable-upsampling">Solution: Learnable Upsampling</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="first-networks"><a class="header" href="#first-networks">First Networks</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="panoptic-segmentation"><a class="header" href="#panoptic-segmentation">Panoptic Segmentation</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="object-detection"><a class="header" href="#object-detection">Object Detection</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="performance-evaluation-intersection-over-union-iou"><a class="header" href="#performance-evaluation-intersection-over-union-iou">Performance Evaluation: Intersection Over Union (IoU)</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sliding-window-detection"><a class="header" href="#sliding-window-detection">Sliding Window Detection</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="part-based-models"><a class="header" href="#part-based-models">Part-Based Models</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deep-learning-in-object-detection"><a class="header" href="#deep-learning-in-object-detection">Deep Learning in Object Detection</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="region-based-cnn-r-cnn"><a class="header" href="#region-based-cnn-r-cnn">Region-Based CNN (R-CNN)</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fast-r-cnn"><a class="header" href="#fast-r-cnn">Fast R-CNN</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="faster-r-cnn"><a class="header" href="#faster-r-cnn">Faster R-CNN</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="feature-pyramid-network-fpn"><a class="header" href="#feature-pyramid-network-fpn">Feature Pyramid Network (FPN)</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mask-r-cnn"><a class="header" href="#mask-r-cnn">Mask R-CNN</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="action-recognition"><a class="header" href="#action-recognition">Action Recognition</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deep-learning-for-video-action-recognition"><a class="header" href="#deep-learning-for-video-action-recognition">Deep Learning for Video Action Recognition</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="single-frame-model"><a class="header" href="#single-frame-model">Single Frame Model</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="multiple-frames"><a class="header" href="#multiple-frames">Multiple Frames</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="limitations-of-feedforward-cnns"><a class="header" href="#limitations-of-feedforward-cnns">Limitations of FeedForward CNNs</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction-of-rnns"><a class="header" href="#introduction-of-rnns">Introduction of RNNs</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="subsequent-developments"><a class="header" href="#subsequent-developments">Subsequent Developments</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="soft-vs-hard-attention"><a class="header" href="#soft-vs-hard-attention">Soft vs Hard Attention</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transformers-in-vision"><a class="header" href="#transformers-in-vision">Transformers in Vision</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transformers-in-computer-vision"><a class="header" href="#transformers-in-computer-vision">Transformers in Computer Vision</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="how-to-pass-an-image-to-a-transformer"><a class="header" href="#how-to-pass-an-image-to-a-transformer">How to Pass an Image to a Transformer?</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transformers-for-object-detection"><a class="header" href="#transformers-for-object-detection">Transformers for Object Detection</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="detr-network-components"><a class="header" href="#detr-network-components">DETR Network Components</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advantages-of-detr"><a class="header" href="#advantages-of-detr">Advantages of DETR</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="disadvantages-of-detr"><a class="header" href="#disadvantages-of-detr">Disadvantages of DETR</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transformers-for-semantic-segmentation"><a class="header" href="#transformers-for-semantic-segmentation">Transformers for Semantic Segmentation</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="segformer-architecture"><a class="header" href="#segformer-architecture">SegFormer Architecture</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advantages-of-segformer"><a class="header" href="#advantages-of-segformer">Advantages of SegFormer</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="swin-transformers"><a class="header" href="#swin-transformers">Swin Transformers</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="architecture-and-key-concepts-of-swin-transformers"><a class="header" href="#architecture-and-key-concepts-of-swin-transformers">Architecture and Key Concepts of Swin Transformers</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advantages-of-swin-transformers"><a class="header" href="#advantages-of-swin-transformers">Advantages of Swin Transformers</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transformers-for-multi-modal-learning"><a class="header" href="#transformers-for-multi-modal-learning">Transformers for Multi-Modal Learning</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="depth-perception"><a class="header" href="#depth-perception">Depth Perception</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="monocular-depth-estimation"><a class="header" href="#monocular-depth-estimation">Monocular Depth Estimation</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="operating-mechanism"><a class="header" href="#operating-mechanism">Operating Mechanism</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="challenges-in-monocular-depth-estimation"><a class="header" href="#challenges-in-monocular-depth-estimation">Challenges in Monocular Depth Estimation</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="monocular-depth-estimation-techniques"><a class="header" href="#monocular-depth-estimation-techniques">Monocular Depth Estimation Techniques</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="applications-of-monocular-depth-estimation"><a class="header" href="#applications-of-monocular-depth-estimation">Applications of Monocular Depth Estimation</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="self-supervised-learning"><a class="header" href="#self-supervised-learning">Self-Supervised Learning</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="main-idea-of-self-supervision"><a class="header" href="#main-idea-of-self-supervision">Main Idea of Self-Supervision</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="key-steps-in-self-supervised-learning"><a class="header" href="#key-steps-in-self-supervised-learning">Key Steps in Self-Supervised Learning</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="examples-of-pretext-tasks"><a class="header" href="#examples-of-pretext-tasks">Examples of Pretext Tasks</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="visual-representation-learning-by-context-prediction"><a class="header" href="#visual-representation-learning-by-context-prediction">Visual Representation Learning by Context Prediction</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="order-recognition"><a class="header" href="#order-recognition">Order Recognition</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="automatic-colorization"><a class="header" href="#automatic-colorization">Automatic Colorization</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="depth-prediction-in-self-supervision"><a class="header" href="#depth-prediction-in-self-supervision">Depth Prediction in Self-Supervision</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advantages-of-self-supervision"><a class="header" href="#advantages-of-self-supervision">Advantages of Self-Supervision</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="contrastive-learning"><a class="header" href="#contrastive-learning">Contrastive Learning</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="key-concepts-in-contrastive-learning"><a class="header" href="#key-concepts-in-contrastive-learning">Key Concepts in Contrastive Learning</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="momentum-contrast-moco"><a class="header" href="#momentum-contrast-moco">Momentum Contrast (MoCo)</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="simclr-simple-framework-for-contrastive-learning-of-visual-representations"><a class="header" href="#simclr-simple-framework-for-contrastive-learning-of-visual-representations">SimCLR (Simple Framework for Contrastive Learning of Visual Representations)</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="barlow-twins"><a class="header" href="#barlow-twins">Barlow Twins</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="key-ideas-of-barlow-twins"><a class="header" href="#key-ideas-of-barlow-twins">Key Ideas of Barlow Twins</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="applications-of-barlow-twins"><a class="header" href="#applications-of-barlow-twins">Applications of Barlow Twins</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advantages-of-barlow-twins"><a class="header" href="#advantages-of-barlow-twins">Advantages of Barlow Twins</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
