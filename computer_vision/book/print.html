<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Computer Vision</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../index.html">Introduction</a></li><li class="chapter-item expanded "><a href="image_processing/introduction.html"><strong aria-hidden="true">1.</strong> Image Processing</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/pointwise_operations/introduction.html"><strong aria-hidden="true">1.1.</strong> Pointwise operations</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/pointwise_operations/linear_pointwise_operations.html"><strong aria-hidden="true">1.1.1.</strong> Linear pointwise operations</a></li><li class="chapter-item expanded "><a href="image_processing/pointwise_operations/non_linear_pointwise_operations.html"><strong aria-hidden="true">1.1.2.</strong> Non linear pointwise operations</a></li></ol></li><li class="chapter-item expanded "><a href="image_processing/neighboring_operations/introduction.html"><strong aria-hidden="true">1.2.</strong> Neighboring operations</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/neighboring_operations/local_histogram_equalization.html"><strong aria-hidden="true">1.2.1.</strong> Local histogram equalization</a></li><li class="chapter-item expanded "><a href="image_processing/neighboring_operations/filters/introduction.html"><strong aria-hidden="true">1.2.2.</strong> Filters</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/neighboring_operations/filters/linear_filters.html"><strong aria-hidden="true">1.2.2.1.</strong> Linear Filters</a></li><li class="chapter-item expanded "><a href="image_processing/neighboring_operations/filters/non_linear_filters.html"><strong aria-hidden="true">1.2.2.2.</strong> Non Linear Filters</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="image_processing/image_resizing/introduction.html"><strong aria-hidden="true">1.3.</strong> Image resizing</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/image_resizing/image_sampling.html"><strong aria-hidden="true">1.3.1.</strong> Image sampling</a></li><li class="chapter-item expanded "><a href="image_processing/image_resizing/image_resampling/introduction.html"><strong aria-hidden="true">1.3.2.</strong> Image resampling</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/image_resizing/image_resampling/down_sampling/introduction.html"><strong aria-hidden="true">1.3.2.1.</strong> Down sampling</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/image_resizing/image_resampling/down_sampling/aliasing.html"><strong aria-hidden="true">1.3.2.1.1.</strong> Aliasing</a></li></ol></li><li class="chapter-item expanded "><a href="image_processing/image_resizing/image_resampling/up_sampling/introduction.html"><strong aria-hidden="true">1.3.2.2.</strong> Up sampling</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/image_resizing/image_resampling/up_sampling/interpolation_techniques.html"><strong aria-hidden="true">1.3.2.2.1.</strong> Interpolation techniques</a></li></ol></li></ol></li></ol></li><li class="chapter-item expanded "><a href="image_processing/image_gradient/introduction.html"><strong aria-hidden="true">1.4.</strong> Image gradient</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/image_gradient/edges/introduction.html"><strong aria-hidden="true">1.4.1.</strong> Edges</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="image_processing/image_gradient/effect_of_noise.html"><strong aria-hidden="true">1.4.1.1.</strong> Effect of Noise</a></li><li class="chapter-item expanded "><a href="image_processing/image_gradient/laplacian_of_gaussian.html"><strong aria-hidden="true">1.4.1.2.</strong> Laplacian of Gaussian</a></li></ol></li></ol></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Computer Vision</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<h2 id="the-need-of-computer-vision"><a class="header" href="#the-need-of-computer-vision">The need of computer vision</a></h2>
<p>The goal of computer vision is to enable computers and systems to derive meaningful information from digital images, videos, and other visual inputs, and to act or make decisions based on that information.
Essentially, it aims to replicate and automate the capabilities of the human visual system.</p>
<h2 id="computer-vision-vs-computer-graphics"><a class="header" href="#computer-vision-vs-computer-graphics">Computer Vision vs. Computer Graphics</a></h2>
<p>Computer Vision is about interpreting and understanding the content of an image or video.
It aims to replicate the human visual system to recognize, detect, and understand objects and scenes in images or videos.</p>
<p>Computer Graphics is the creation and manipulation of images and videos using computers.
It focuses on generating visual content through rendering, animation, and visualization techniques.
Essentially, while computer vision is about understanding images, computer graphics is about creating them.</p>
<h2 id="computer-vision-vs-image-processing"><a class="header" href="#computer-vision-vs-image-processing">Computer Vision vs. Image Processing</a></h2>
<p>Image Processing involves performing operations on images to enhance them or extract useful information.
It deals with basic transformations like resizing, filtering, and color adjustments, and is often used as a preprocessing step in computer vision workflows to improve the quality of input data for better analysis.</p>
<p>Computer Vision uses the outputs of image processing to perform more complex tasks such as object recognition, scene reconstruction, and more.
It goes beyond processing the image to understanding the context and elements within the visual data.</p>
<h2 id="computer-vision-vs-deep-learning"><a class="header" href="#computer-vision-vs-deep-learning">Computer Vision vs. Deep Learning</a></h2>
<p>Deep Learning is a subset of machine learning that uses neural networks with many layers (hence "deep") to learn from a vast amount of data.
Deep learning algorithms, particularly Convolutional Neural Networks (CNNs), have become fundamental in modern computer vision tasks, enabling advancements in recognition, detection, and classification.</p>
<p>Computer Vision is one of the fields that has greatly benefited from deep learning, applying these algorithms to achieve significant improvements in visual understanding and analysis.
However, computer vision is not limited to deep learning alone and includes a variety of techniques from image processing, pattern recognition, and geometric modeling.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-processing"><a class="header" href="#image-processing">Image Processing</a></h1>
<p>A general image processing operator is a function that takes one or more input images and procudes an output image.</p>
<p>An image can be thought as a function $f$ from $\mathcal{R}^2$ to $\mathcal{R}^3$.
Assume you have a point $p \in \mathcal{R}^2$:</p>
<p>$$
f(p) = \begin{bmatrix} r(p) \ g(p) \ b(p) \end{bmatrix}
$$</p>
<p>Where $r,g$ and $b$ are rispectively the reed, green and blue components of the image.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pointwise-operations"><a class="header" href="#pointwise-operations">Pointwise operations</a></h1>
<p>Pointwise operations are applied at a pixel level and the output only depends on the input pixel and some additional informations.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="linear-pointwise-operations"><a class="header" href="#linear-pointwise-operations">Linear pointwise operations</a></h1>
<p>A common linear point process is multiplication and addition with a constant, which regulates the contrast and the brightness respectively.</p>
<p>Another linear operation is the linear blend, which is used to create a temporal cross-dissolve effect between two images or videos.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="non-linear-pointwise-operations"><a class="header" href="#non-linear-pointwise-operations">Non linear pointwise operations</a></h1>
<h2 id="gamma-corection"><a class="header" href="#gamma-corection">Gamma corection</a></h2>
<p>Gamma correction is a non linear transform that is often applied to images before further processing.</p>
<p>$$
p_{out} = p_{in}^{\gamma}
$$</p>
<p>Where $\gamma \approx 2.2$, which is in general not a good approximation but works well in practice.
Gamma correction is used to align digital images to human visual perception, improving image quality and giving the image consistency accross different devices.</p>
<h2 id="histrogram-equalization"><a class="header" href="#histrogram-equalization">Histrogram equalization</a></h2>
<p>The histogram of each chanel and luminance values of an image describes the set of intensity values in an image.
From the histogram of an image we can compute relevan statistics for the image, in particular we can determine the pixel intensity distribution.</p>
<p>Histogram equalization is a technique used to automatically determine the best contrast values in an image by adjusting the distribution of pixel instensity values of an image such that the histogram of pixel instensities is more uniform across the possible instisity values.</p>
<p>Histogram equalization is a global operation because it considers the distribution of pixel intensities across the entire image.
The adjustment made to each pixel's intensity is based on the cumulative distribution function (CDF) derived from the global histogram of all the pixel values in the image.
The histogram equalization is considered a pointwise operation because each pixel's new value is determined solely by its original value, without considering the values of neighboring pixels.
The mapping function, derived from the CDF, is applied directly to each pixel to determine its new intensity.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="neighboring-operations"><a class="header" href="#neighboring-operations">Neighboring operations</a></h1>
<p>Neighbourhood operations are a generalization of the point operations.
A pixel in the processed image now depends not only on the corresponding pixel in the input image but also its neighbouring pixels.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="local-histogram-equalization"><a class="header" href="#local-histogram-equalization">Local histogram equalization</a></h1>
<p>Local histogram equalization is a technique used to enhance the contrast of images, similar to global histogram equalization, but with a key difference: it operates on small, localized regions of the image rather than on the entire image.</p>
<p>This method is particularly useful for improving the visibility of features in images that have varying lighting conditions across different areas.</p>
<p>In local histogram equalization, the resulting intensity values for a pixel are determined by the histogram of the region it belongs to, making it a neighborhood operation.
Each pixel's new intensity depends on the specific distribution of intensities within its local neighborhood or region, as defined by the small tiles or windows used in the process.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="filters"><a class="header" href="#filters">Filters</a></h1>
<p>Filters are types of neighborhood operations designed to modify or enhance images through a defined set of rules or kernels.
Filters can be linear (like Gaussian or averaging filters) or non-linear (like median filters), each tailored to specific tasks such as smoothing, sharpening, or noise reduction.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="linear-filters"><a class="header" href="#linear-filters">Linear Filters</a></h1>
<p>Linear Filters apply a linear operation to the pixels in a neighborhood defined by a kernel or a mask.
The new value of a target pixel is computed as a weighted sum of the pixel values within the kernel's footprint.</p>
<p>Linear filters are typically applied using a process called convolution, which is closely related to another operation known as cross-correlation.
Both convolution and cross-correlation involve sliding a kernel (or filter) over the image and computing the sum of element-wise products at each position.
The subtle difference between these two operations primarily lies in the orientation of the kernel:</p>
<ul>
<li>
<p>Cross-Correlation: In cross-correlation, the kernel is slid over the image as it is, without flipping. This means the top-left value of the kernel always multiplies the top-left value of the image region it covers.</p>
</li>
<li>
<p>Convolution: Convolution involves flipping the kernel both horizontally and vertically before sliding it over the image. This means that the operation is a more complex form of template matching, as it takes into account the spatial orientation of the kernel's weights relative to the target feature in the image.</p>
</li>
</ul>
<p>Convolution is used in signal processing and image processing because it mathematically models the way physical systems respond to stimuli.
In practical image processing, especially with symmetric kernels (like Gaussian blurs), convolution and cross-correlation can produce the same results because the flipping of a symmetric kernel does not change its layout.</p>
<p>Convolution is a key operation used in convolutional neural networks (CNNs), including famous architectures like AlexNet.
CNNs fundamentally use a sequence of learned linear filters (convolutional layers) to process the input images.
This operation allows CNNs to effectively learn features from image data, making them particularly well-suited for tasks like image classification, object detection, and more.
Convolutional layers in a CNN use learned filters (kernels) to convolve across the input image or feature maps from previous layers.
Each filter is designed (or learned during training) to detect specific features such as edges, textures, or more complex patterns depending on the depth in the network.
As the input data passes through successive convolutional layers, the network can form a hierarchy of features from simple to complex.
Lower layers typically detect simple features (e.g., edges and corners), while deeper layers combine these features to detect higher-level structures (e.g., parts of objects or entire objects).</p>
<p>In the context of convolutional neural networks (CNNs), padding and stride are crucial parameters that influence how the convolution operation is applied to the input image or feature maps. Both settings help control the spatial dimensions of the output feature maps and can have a significant impact on the network's performance and efficiency.</p>
<p>Padding refers to the practice of adding layers of zeros (or other values) around the edge of the input image or feature map before applying the convolution operation.
Without padding, the spatial dimensions of the output feature map are reduced with each convolutional layer (unless a stride of 1 is used and the kernel size is 1x1).
Padding allows the convolution operation to cover the bordering elements of the input, enabling the output feature map to maintain the same size as the input.
This is particularly important in deep networks, where many convolutional layers would otherwise progressively shrink the spatial dimensions of the feature maps to a point where no further convolutions could be applied.
Without padding, pixels on the border of the image are used less frequently than pixels in the center when applying filters.
Padding increases the number of times edge pixels are used in convolution computations, helping the model learn from the entire image more effectively.</p>
<p>Stride dictates the number of pixels the convolution filter moves across the input image or feature map after each operation.
A stride greater than one reduces the spatial dimensions of the output feature map.
This is because the filter skips over pixels as it slides across the input.
Using a stride of 2, for example, typically reduces the dimensions of the output feature map to half those of the input, assuming other settings remain constant.
Increasing the stride reduces the computational load and the size of the output, which can speed up the training and inference processes.
A higher stride increases the effective field of view of each application of the convolution kernel, allowing it to cover a broader area of the input with fewer operations.
This can help the network capture more global features faster, though it may reduce the granularity of the feature maps.</p>
<h2 id="gaussian-filters"><a class="header" href="#gaussian-filters">Gaussian Filters</a></h2>
<p>Gaussian filters are a type of image smoothing filter that reduce noise and detail in images using a Gaussian function.
They are characterized by their bell-shaped curve in one dimension and by a surface whose sections are Gaussian curves in two dimensions.
Gaussian filters are widely used due to their properties in the spatial and frequency domains.
Due to its nature, the Gaussian filter provides smooth gradients without sharp transitions, making it ideal for blurring and for use in scale-space representation.
In two dimensions, a Gaussian filter is circularly symmetric (isotropic), meaning it blurs uniformly in all directions.
Gaussian filters have a low-pass characteristic, which means they attenuate high-frequency components more than low-frequency components, effectively reducing image noise and detail.</p>
<p>A separable filter is a type of filter that can be broken down into the product of two or more one-dimensional kernels.
This allows the two-dimensional convolution operation to be performed more efficiently by reducing it to multiple one-dimensional convolutions.</p>
<p>Gaussian filters are inherently separable, which is one of their advantageous properties.
A two-dimensional Gaussian kernel can be expressed as the outer product of two identical one-dimensional Gaussian kernels.
This means that a 2D Gaussian blurring operation can be efficiently implemented by first applying a 1D Gaussian blur vertically across the image and then horizontally.
This separability makes Gaussian filters particularly appealing for real-time processing and applications in computer vision where computational resources are a concern.</p>
<h2 id="sharpen-filter"><a class="header" href="#sharpen-filter">Sharpen Filter</a></h2>
<p>Sharpening filters are used in image processing to enhance the visibility of edges and fine details in images.
They work by emphasizing high-frequency components, which correspond to rapid changes in image intensity, such as edges.
This is typically achieved through a process that enhances the contrast at these high-frequency locations.</p>
<p>A common approach involves using a kernel that accentuates edges.
This can be done with kernels that approximate the second derivative of the image (like the Laplacian filter), or by simply using a kernel that boosts center pixel intensity while subtracting a fraction of the neighboring pixel intensities.</p>
<p>Gaussian filters are inherently smoothing filters and are typically used to reduce noise and detail in the images.
When used as part of an unsharp masking technique, the Gaussian filter serves to isolate the low-frequency components by smoothing out the high frequencies.
The subtractive process in unsharp masking that follows the Gaussian blurring emphasizes the high-frequency components (edges) by reducing the weight of low frequencies.
This process effectively increases the sharpness of the image.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="non-linear-filters"><a class="header" href="#non-linear-filters">Non Linear Filters</a></h1>
<p>Non-Linear Filters involve operations where the output is not a linear combination of the input pixel values. The operation may include conditions, thresholds, or more complex relationships that do not satisfy linearity.</p>
<h2 id="median-filters"><a class="header" href="#median-filters">Median Filters</a></h2>
<p>Median filters are a type of non-linear digital filtering technique, often used to remove noise from an image or signal.
The median filter is particularly effective at removing 'salt and pepper' noise while preserving edges in an image.
The median filter operates by sliding a window (kernel) over each pixel of the image. For each position of the window, the pixel values covered by the window are sorted numerically. The median value of the sorted pixels is then determined, and this median value replaces the pixel value at the center of the window.
Unlike mean filtering, median filtering does not blur the edges, as the median is less sensitive than the mean to extreme values (which are often edge pixels).</p>
<h2 id="bilateral-filter"><a class="header" href="#bilateral-filter">Bilateral Filter</a></h2>
<p>Bilateral filtering is an advanced method of image smoothing that provides edge preservation while reducing noise, addressing a common limitation found in traditional Gaussian filters.</p>
<p>Gaussian filters are excellent for blurring and noise reduction in images because they effectively smooth variations in intensity.
They work by applying a Gaussian kernel to the image, which averages the pixel values in a way that nearby pixels have a larger influence on the output than distant ones.
However, this isotropic smoothing does not discriminate between edges and noise; it blurs everything indiscriminately, including important edge details.
As a result, while Gaussian filters reduce noise, they also tend to blur sharp edges, which can be undesirable in applications where maintaining edge clarity is important.</p>
<p>Bilateral filtering was introduced to overcome the edge-blurring problem of Gaussian filters.
It does this by taking both spatial proximity and the intensity similarity into account when performing the smoothing:</p>
<ul>
<li>
<p>Spatial Component: Like Gaussian filtering, bilateral filtering considers the closeness of pixels. This is typically modeled using a Gaussian distribution that decreases the weights of pixels based on their spatial distance from the target pixel.</p>
</li>
<li>
<p>Range Component: Unlike Gaussian filtering, bilateral filtering also considers the similarity in intensity values. This range filter ensures that only pixels with intensity values similar to the target pixel are considered for averaging. This component is also typically modeled using a Gaussian distribution, but instead of spatial distance, it uses the intensity difference.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-resizing"><a class="header" href="#image-resizing">Image resizing</a></h1>
<p>Image resizing is a fundamental operation in image processing that involves changing the dimensions of an image.
This can mean scaling up to make the image larger, or scaling down to make it smaller.
Effective image resizing techniques are crucial for a wide range of applications, from multimedia processing to machine learning, where images need to be normalized to a consistent size.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-sampling"><a class="header" href="#image-sampling">Image sampling</a></h1>
<p>Sampling refers to the process of selecting a subset of data from a continuous signal or from a larger set of data points. In the context of image processing, this often relates to the initial acquisition of digital data from an analog source (like converting light captured by a camera sensor into pixel values) or selecting specific pixels or regions from an existing digital image for analysis or processing.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-resampling"><a class="header" href="#image-resampling">Image resampling</a></h1>
<p>Resampling is about modifying an existing dataset (changing the pixel grid of an image) through processes like interpolation or decimation.
This typically involves creating a new set of data points from the original dataset under conditions where you're changing the grid, spacing, or orientation of the data.</p>
<p>Resampling is used when the existing dataset’s spatial arrangement doesn’t fit the needed application, such as when scaling images to a new resolution, rotating them, or applying other geometric transformations that alter their original grid structure.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="down-sampling"><a class="header" href="#down-sampling">Down sampling</a></h1>
<p>Downsampling is a resampling process that involves reducing the number of pixels, commonly for reducing file size, removing redundant information, or adjusting the image to a lower resolution display.
Downsampling also uses methods like averaging or more complex filters to decide the value of new, fewer pixels based on the original pixel grid.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="aliasing"><a class="header" href="#aliasing">Aliasing</a></h1>
<p>Aliasing is a phenomenon that occurs when a signal is sampled without sufficient resolution to accurately capture its high-frequency components.
In the context of image processing, aliasing manifests as visual artifacts—such as jagged edges or moiré patterns—when high-frequency details (fine textures or sharp transitions) are not adequately represented due to undersampling.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="up-sampling"><a class="header" href="#up-sampling">Up sampling</a></h1>
<p>Upsampling is a specific type of resampling where the goal is to increase the number of pixels in an image, typically to make the image larger or to match a certain resolution requirement.
This is done through interpolation methods such as nearest neighbor, bilinear, bicubic, etc., which calculate and fill in the pixel values that didn’t exist in the original image.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="interpolation-techniques"><a class="header" href="#interpolation-techniques">Interpolation techniques</a></h1>
<h2 id="nearest-neighbor"><a class="header" href="#nearest-neighbor">Nearest Neighbor</a></h2>
<p>Nearest neighbor interpolation is a method of upsampling that interpolates new pixel values depending only on the closest pixels.
Is one of the fastes and simplest methods that can be used.</p>
<h2 id="bilinear-interpolation"><a class="header" href="#bilinear-interpolation">Bilinear interpolation</a></h2>
<p>Bilinear interpolation is a resampling method used in image processing to determine the intensity of a new pixel based on a weighted average of the $4$ nearest pixels in the original image.</p>
<h2 id="bicubic-interpolation"><a class="header" href="#bicubic-interpolation">Bicubic interpolation</a></h2>
<p>Bilinear interpolation is a resampling method used in image processing to determine the intensity of a new pixel based on a weighted average of the $16$ nearest pixels in the original image.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-gradient"><a class="header" href="#image-gradient">Image gradient</a></h1>
<p>An image gradient is a directional change in the intensity or color in an image.
It measures how the image intensity changes between adjacent pixels and is used to find edges and other significant transitions in images.</p>
<p>The gradient of an image is calculated by taking derivatives in the horizontal and vertical directions.
This is usually done by convolving the image with derivative filters (kernels).</p>
<p>The gradient consists of two components:</p>
<ul>
<li>Horizontal Gradient ([G_x]): Measures changes in intensity in the horizontal direction.</li>
<li>Vertical Gradient ([G_y]): Measures changes in intensity in the vertical direction</li>
</ul>
<p>Partial derivatives represent the rate of change of image intensities with respect to the [x] and [y] directions independently.
They help in understanding the variation of pixel values along each dimension of the image.</p>
<p>Gradient magnitude is a measure of the strength of the gradient at each pixel.
It gives the rate at which the pixel values change at that point, and thus is indicative of the presence of edges or boundaries.
High values typically indicate strong edges where the intensity changes abruptly, while low values might suggest flat areas with little to no change in intensity.</p>
<p>$$
Magnitude (G) = sqrt(G_x^2 + G_y^2)
$$</p>
<p>Gradient Orientation indicates the direction of the greatest rate of increase of intensity from one pixel to another.
It points perpendicular to the edge direction.
Provides the angle at which these changes occur, which can be crucial for algorithms that need to understand the structure or layout of features within an image, such as in texture analysis or object recognition.</p>
<p>$$
Orientation (θ) = atan2(G_y, G_x)
$$</p>
<p>Typically, partial derivatives are computed using convolution with derivative kernels such as the Sobel, Prewitt, or Scharr operators, which are designed to emphasize changes in pixel values along one dimension while averaging out changes in the perpendicular dimension.</p>
<h2 id="gradient-estimation-methods"><a class="header" href="#gradient-estimation-methods">Gradient estimation methods</a></h2>
<ul>
<li>Sobel: The Sobel operator is one of the most widely used methods for gradient calculation in image processing.
It uses two separate convolution kernels, one for detecting horizontal changes (Gx) and one for vertical changes (Gy).</li>
<li>The Prewitt operator is similar to the Sobel operator but uses different convolution kernels that do not emphasize the pixels directly adjacent to the central pixel.
-Scharr: The Scharr operator offers an alternative that optimizes the rotation invariance in gradient calculation. Its coefficients provide a better approximation to derivative computation.
-Robers: The Roberts Cross operator calculates the gradient using a pair of 2x2 convolution kernels. It is particularly effective at highlighting diagonal edges.</li>
</ul>
<h2 id="edges"><a class="header" href="#edges">Edges</a></h2>
<p>Edges can be characterized as the points in an image at which the image brightness changes abruptly.
These changes usually represent the boundaries of objects, texture changes, or other significant variations in the image scene.</p>
<p>Edges are important features in visual data because they often define the boundaries of objects within an image, making them crucial for various tasks in image processing and computer vision, such as segmentation and object recognition.</p>
<p>Image gradients are directly related to edge detection because they highlight areas of the image with significant intensity changes, which correspond to edges:</p>
<ul>
<li>
<p>Gradient Magnitude: High gradient magnitudes indicate strong intensity changes and are therefore likely locations of edges. Wherever the gradient magnitude is high, there is likely an edge or a boundary.</p>
</li>
<li>
<p>Gradient Orientation: The orientation of the gradient at each pixel points in the direction where the intensity increases most rapidly and is perpendicular to the direction of the edge. Thus, knowing the gradient orientation can help in tracing the direction or alignment of edges within an image.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="edges-1"><a class="header" href="#edges-1">Edges</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="effect-of-noise"><a class="header" href="#effect-of-noise">Effect of Noise</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="laplacian-of-gaussian"><a class="header" href="#laplacian-of-gaussian">Laplacian of Gaussian</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
